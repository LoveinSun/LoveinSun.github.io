{"meta":{"title":"JugglerDancing","subtitle":"Juggler is dancing","description":"吾之生命如流星，誓要从全世界路过。","author":"Juggler","url":"https://juggler.fun","root":"/"},"pages":[{"title":"Tags","date":"2022-03-22T10:27:44.000Z","updated":"2022-03-22T10:29:38.008Z","comments":true,"path":"Tags/index.html","permalink":"https://juggler.fun/Tags/index.html","excerpt":"","text":""},{"title":"Categories","date":"2022-03-22T10:29:15.000Z","updated":"2022-03-22T10:30:10.272Z","comments":true,"path":"Categories/index.html","permalink":"https://juggler.fun/Categories/index.html","excerpt":"","text":""}],"posts":[{"title":"日本語","slug":"日本語","date":"2022-03-24T08:13:21.000Z","updated":"2022-03-24T08:56:53.173Z","comments":true,"path":"language/日本語/","link":"","permalink":"https://juggler.fun/language/%E6%97%A5%E6%9C%AC%E8%AA%9E/","excerpt":"","text":"第二课 はじめまして - あ い う え お が が ぎ ぐ げ ご ざ ざ じ ず ぜ ぞ だ だ ぢ で ど ば ば び ぶ べ ぼ","categories":[{"name":"language","slug":"language","permalink":"https://juggler.fun/categories/language/"}],"tags":[{"name":"janpanese","slug":"janpanese","permalink":"https://juggler.fun/tags/janpanese/"}]},{"title":"Data","slug":"Data","date":"2022-03-24T07:42:39.000Z","updated":"2022-03-24T07:43:32.515Z","comments":true,"path":"data/Data/","link":"","permalink":"https://juggler.fun/data/Data/","excerpt":"","text":"nft: https://nftschool.dev","categories":[{"name":"data","slug":"data","permalink":"https://juggler.fun/categories/data/"}],"tags":[{"name":"data","slug":"data","permalink":"https://juggler.fun/tags/data/"}]},{"title":"Unity fake parent","slug":"Unity-fake-parent","date":"2022-03-23T08:25:36.000Z","updated":"2022-03-23T08:26:30.068Z","comments":true,"path":"Unity/Unity-fake-parent/","link":"","permalink":"https://juggler.fun/Unity/Unity-fake-parent/","excerpt":"","text":"Unity 设置不是父子关系的跟随效果 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758using System;using Entity;using UnityEngine;public class Follow : MonoBehaviour&#123; public Transform FakeParent; private Vector3 _positionOffset; private Quaternion _rotationOffset; private void Start() &#123; if (FakeParent != null) &#123; SetFakeParent(FakeParent); &#125; //Invoke(&quot;Timer&quot;, 15.0f); &#125; private void Update() &#123; if (FakeParent == null) return; var targetPos = FakeParent.position - _positionOffset; var targetRot = FakeParent.localRotation * _rotationOffset; transform.position = RotatePointAroundPivot(targetPos, FakeParent.position, targetRot); transform.localRotation = targetRot; &#125; void Timer() &#123; //Debug.Log(string.Format(&quot;Timer3 is up !!! time=$&#123;0&#125;&quot;, Time.time)); //GC.Collect(); //Invoke(&quot;Timer&quot;, 17.0f); &#125; public void SetFakeParent(Transform parent) &#123; //Offset vector _positionOffset = parent.position - transform.position; //Offset rotation _rotationOffset = Quaternion.Inverse(parent.localRotation * transform.localRotation); //Our fake parent FakeParent = parent; &#125; public Vector3 RotatePointAroundPivot(Vector3 point, Vector3 pivot, Quaternion rotation) &#123; //Get a direction from the pivot to the point Vector3 dir = point - pivot; //Rotate vector around pivot dir = rotation * dir; //Calc the rotated vector point = dir + pivot; //Return calculated vector return point; &#125;&#125;","categories":[{"name":"Unity","slug":"Unity","permalink":"https://juggler.fun/categories/Unity/"}],"tags":[{"name":"Unity","slug":"Unity","permalink":"https://juggler.fun/tags/Unity/"}]},{"title":"碎碎念","slug":"碎碎念","date":"2022-03-23T06:04:05.000Z","updated":"2022-03-23T06:04:45.674Z","comments":true,"path":"杂记/碎碎念/","link":"","permalink":"https://juggler.fun/%E6%9D%82%E8%AE%B0/%E7%A2%8E%E7%A2%8E%E5%BF%B5/","excerpt":"","text":"","categories":[{"name":"杂记","slug":"杂记","permalink":"https://juggler.fun/categories/%E6%9D%82%E8%AE%B0/"}],"tags":[{"name":"杂记","slug":"杂记","permalink":"https://juggler.fun/tags/%E6%9D%82%E8%AE%B0/"}]},{"title":"小记","slug":"小记","date":"2022-03-23T03:07:16.000Z","updated":"2022-03-23T08:28:23.017Z","comments":true,"path":"杂记/小记/","link":"","permalink":"https://juggler.fun/%E6%9D%82%E8%AE%B0/%E5%B0%8F%E8%AE%B0/","excerpt":"","text":"待我低吟一首小诗，韵了平仄，与你细说，世间有此河，躺世俗尸骨，仙胎无计数。","categories":[{"name":"杂记","slug":"杂记","permalink":"https://juggler.fun/categories/%E6%9D%82%E8%AE%B0/"}],"tags":[{"name":"杂记","slug":"杂记","permalink":"https://juggler.fun/tags/%E6%9D%82%E8%AE%B0/"}]},{"title":"js promise 牛刀小试","slug":"js-promise-牛刀小试","date":"2022-03-23T02:59:45.000Z","updated":"2022-03-23T03:24:31.744Z","comments":true,"path":"js/js-promise-牛刀小试/","link":"","permalink":"https://juggler.fun/js/js-promise-%E7%89%9B%E5%88%80%E5%B0%8F%E8%AF%95/","excerpt":"","text":"123456789101112131415161718192021fuck:function () &#123; this.timeOut().then(function success(value)&#123; console.log(value) &#125;) &#125;, timeOut:function ()&#123; return new Promise(function (resolve, reject)&#123; console.log(&quot;WTF&quot;) setTimeout(function()&#123; try &#123; console.log(&quot;WTF2&quot;) resolve(&quot;成功!&quot;);//代码正常执行！ console.log(&quot;WTF3&quot;) &#125;catch (e) &#123; reject(e); &#125; &#125;, 2000); &#125;); &#125;,","categories":[{"name":"js","slug":"js","permalink":"https://juggler.fun/categories/js/"}],"tags":[{"name":"js","slug":"js","permalink":"https://juggler.fun/tags/js/"}]},{"title":"python 利用azcopy 从azure storage 上下载内容","slug":"python-利用azcopy-从azure-storage-上下载内容","date":"2022-03-23T02:56:20.000Z","updated":"2022-03-23T03:22:05.936Z","comments":true,"path":"python/python-利用azcopy-从azure-storage-上下载内容/","link":"","permalink":"https://juggler.fun/python/python-%E5%88%A9%E7%94%A8azcopy-%E4%BB%8Eazure-storage-%E4%B8%8A%E4%B8%8B%E8%BD%BD%E5%86%85%E5%AE%B9/","excerpt":"","text":"1234567891011121314151617181920212223242526from LRUCache import LRUCacheimport subprocess def get_model(model_path): print(&quot;get model&quot;) if model_path == &quot;&quot;: return None; model = cache.get(model_path) if model!=-1: return model local_path = head+model_path.split(&quot;/&quot;)[-1] print(f&quot;input model_path:&#123;model_path&#125;\\nlocal_path:&#123;local_path&#125;&quot;) print(1) cmd_ret = None print(os.path.exists(local_path)) if not os.path.exists(local_path): index_cmd_return = subprocess.run([&quot;./azcopy&quot;, &quot;copy&quot;, url_template.format(model_path),head,&quot;--recursive&quot;]) model = load_model(local_path) print(2) cache.set(model_path,model) return model get_model(&quot;ao/als/aia-tra/b/model_epoch5-it235-loss1.514&quot;)","categories":[{"name":"python","slug":"python","permalink":"https://juggler.fun/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"https://juggler.fun/tags/python/"},{"name":"azure","slug":"azure","permalink":"https://juggler.fun/tags/azure/"}]},{"title":"python MongoDB","slug":"python-MongoDB","date":"2022-03-23T02:54:39.000Z","updated":"2022-03-23T03:20:47.966Z","comments":true,"path":"python/python-MongoDB/","link":"","permalink":"https://juggler.fun/python/python-MongoDB/","excerpt":"","text":"12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788from pymongo import MongoClient class User: FirstName = None LastName = None Email = None Password = None def __init__(self,Email,Password,FirstName,LastName = &quot;&quot;): self.Email = Email self.Password = Password self.FirstName = FirstName self.LastName = LastName def json(self): json = &#123; &quot;Email&quot;:self.Email, &quot;Password&quot;:self.Password, &quot;FirstName&quot;:self.FirstName, &quot;LastName&quot;:self.LastName &#125; return json class GameDB: url = &quot;&quot; client = None db = None def __init__(self,url = &quot;mongodb://jo:jokda@localhost&quot;): self.url = url self.client = MongoClient(self.url) self.db = self.client.game def insert(self,user,flag=False): if flag: self.db.inventory.insert_many(user) return 1 select = self.select(&#123;&quot;Email&quot;:user.Email&#125;) if len(select) != 0 : return -1 json_user = user.json() self.db.inventory.insert_one(json_user) return 1 def select(self,condition=&#123;&#125;): cursor = self.db.inventory.find(condition) result = [doc for doc in cursor] return result def update(self,user:User,flag=True): select = self.select(&#123;&quot;Email&quot;:user.Email&#125;) if len(select)&lt;1: return -1 if flag : self.db.inventory.update_many( &#123;&quot;Email&quot;:user.Email&#125;, &#123;&quot;$set&quot;:&#123; &quot;Password&quot;:user.Password, &quot;FirstName&quot;:user.FirstName, &quot;LastName&quot;:user.LastName &#125;&#125;, ) return 1 self.db.inventory.update_one( &#123;&quot;Email&quot;:user.Email&#125;, &#123;&quot;$set&quot;:&#123; &quot;$Password&quot;:user.Password, &quot;$FirstName&quot;:user.FirstName, &quot;$LastName&quot;:user.LastName &#125;&#125;, ) return 1 def delete(self,condition,flag=True): if flag : self.db.inventory.delete_many(condition) else: self.db.inventory.delete_one(condtion) return 1 #user = User(&quot;19@qq.com&quot;,&quot;password&quot;,&quot;pa&quot;,&quot;zika&quot;) #db = GameDB()#db.add(user)#db.printf()","categories":[{"name":"python","slug":"python","permalink":"https://juggler.fun/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"https://juggler.fun/tags/python/"},{"name":"MongoDB","slug":"MongoDB","permalink":"https://juggler.fun/tags/MongoDB/"}]},{"title":"python LRUCache","slug":"python-LRUCache","date":"2022-03-23T02:53:42.000Z","updated":"2022-03-23T03:21:32.411Z","comments":true,"path":"python/python-LRUCache/","link":"","permalink":"https://juggler.fun/python/python-LRUCache/","excerpt":"","text":"1234567891011121314151617181920212223242526272829303132333435363738394041class LRUCache(collections.OrderedDict): def __init__(self, size=2): #print(self.size) self.size = size, print(self.size) self.cache = collections.OrderedDict() def get(self, key): #if self.cache.has_key(key): if key in self.cache: val = self.cache.pop(key) self.cache[key] = val else: val = -1 return val def set(self, key, val): print(f&quot;len &#123;len(self.cache)&#125;,size&#123;self.size&#125;&quot;) #if self.cache.has_key(key): if key in self.cache: val = self.cache.pop(key) self.cache[key] = val else: if len(self.cache) &gt;= self.size[0]: item = self.cache.popitem(last=False) delete(item) self.cache[key] = val def delete(item): print(&quot;============================start delete=========================================&quot;) print(item) #del item[1] path =&quot;/&quot;.join(item[0].split(&quot;/&quot;)[:-3]) print(&quot;delete path : &quot;+path) subprocess.run([&quot;rm&quot;,&quot;-r&quot;,path]) del item gc.collect() print(&quot;============================delete comlete=======================================&quot;)","categories":[{"name":"python","slug":"python","permalink":"https://juggler.fun/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"https://juggler.fun/tags/python/"}]},{"title":"C# AES加盐加密 python AES加盐解密","slug":"C-AES加盐加密-python-AES加盐解密","date":"2022-03-23T02:51:46.000Z","updated":"2022-03-23T03:25:15.758Z","comments":true,"path":"密码学/C-AES加盐加密-python-AES加盐解密/","link":"","permalink":"https://juggler.fun/%E5%AF%86%E7%A0%81%E5%AD%A6/C-AES%E5%8A%A0%E7%9B%90%E5%8A%A0%E5%AF%86-python-AES%E5%8A%A0%E7%9B%90%E8%A7%A3%E5%AF%86/","excerpt":"","text":"C# 12345678910111213141516171819202122232425262728static string encrypt(string clearText = &quot;&quot;) &#123; if (clearText == null) &#123; clearText = &quot;&quot;; &#125; string EncryptionKey = &quot;Saturday&quot;; byte[] clearBytes = Encoding.UTF8.GetBytes(clearText); using (Aes encryptor = Aes.Create()) &#123; Rfc2898DeriveBytes pdb = new Rfc2898DeriveBytes(EncryptionKey, new byte[] &#123; 0x49, 0x76, 0x61, 0x6e, 0x20, 0x4d, 0x65, 0x64, 0x76, 0x65, 0x64, 0x65, 0x76 &#125;, 100000, HashAlgorithmName.SHA1); encryptor.Key = pdb.GetBytes(32); encryptor.IV = pdb.GetBytes(16); encryptor.Mode = CipherMode.CBC; using (MemoryStream ms = new MemoryStream()) &#123; using (CryptoStream cs = new CryptoStream(ms, encryptor.CreateEncryptor(), CryptoStreamMode.Write)) &#123; cs.Write(clearBytes, 0, clearBytes.Length); cs.Close(); &#125; clearText = Convert.ToBase64String(ms.ToArray()); &#125; &#125; return clearText; &#125; 1234567891011121314151617181920def Decryptstr(text): try: if text is None: return else: backend = default_backend() EncryptionKey = &quot;Saturday&quot; salt = bytes([ 0x49, 0x76, 0x61, 0x6e, 0x20, 0x4d, 0x65, 0x64, 0x76, 0x65, 0x64, 0x65, 0x76 ]) kdf = PBKDF2HMAC(algorithm=hashes.SHA1(),length=48,salt=salt,iterations=100000,backend=backend) key_bytes = kdf.derive(bytes(EncryptionKey, &#x27;utf-8&#x27;)) key = key_bytes[0:32] iv = key_bytes[32:] cipherbytes = base64.b64decode(text) cipher = AES.new(key, AES.MODE_CBC, iv) password = cipher.decrypt(cipherbytes).decode(&#x27;utf-8&#x27;) return password except Exception as err: print(err)","categories":[{"name":"密码学","slug":"密码学","permalink":"https://juggler.fun/categories/%E5%AF%86%E7%A0%81%E5%AD%A6/"}],"tags":[{"name":"C#","slug":"C","permalink":"https://juggler.fun/tags/C/"},{"name":"python","slug":"python","permalink":"https://juggler.fun/tags/python/"},{"name":"密码学","slug":"密码学","permalink":"https://juggler.fun/tags/%E5%AF%86%E7%A0%81%E5%AD%A6/"}]},{"title":"fastapi nacos","slug":"fastapi-nacos","date":"2022-03-23T02:25:44.000Z","updated":"2022-03-23T03:26:29.785Z","comments":true,"path":"nacos/fastapi-nacos/","link":"","permalink":"https://juggler.fun/nacos/fastapi-nacos/","excerpt":"","text":"123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081import nacos_pyfrom fastapi import FastAPIimport yamlfrom munch import munchifyfrom starlette.requests import Requestfrom starlette.responses import Responseimport tracebackimport sysimport warnings sys.tracebacklimit = 3 # 限制打印错误行数# 首先配置application.properties中nacos.core.auth.enabled=true# 配置nacos 用户-角色-权限 app = FastAPI() # async def catch_exceptions_middleware(request: Request, call_next):# try:# return await call_next(request)# except Exception as e:# # print(e)# # print(traceback.format_list())# return Response(&quot;Internal server error&quot;, status_code=500)### app.middleware(&#x27;http&#x27;)(catch_exceptions_middleware) @app.exception_handler(Exception)async def custom_http_exception_handler(request, exc): return Response(&quot;Internal server exception&quot;, status_code=500) @app.exception_handler(ZeroDivisionError) # 统一处理某些错误async def custom_http_exception_handler(request, exc): return Response(&quot;Internal server error&quot;, status_code=500) class nacos: SERVER_ADDRESSES = &quot;127.0.0.1:8845&quot; NAMESPACE = &quot;public&quot; client = nacos_py.NacosClient(SERVER_ADDRESSES, namespace=NAMESPACE, username=&quot;pang&quot;, password=&quot;pang&quot;) data_id = &quot;testDataId&quot; group = &quot;testGroup&quot; def add_watcher(self, methods): self.client.add_config_watchers(self.data_id, self.group, methods) def get_config(self, data_id=None, group=None): if data_id is None: data_id = self.data_id if group is None: group = self.group confi = self.client.get_config(data_id, group) confi = yaml.safe_load(confi) core_config = munchify(confi) print(core_config) return confi def call(args): global config config = a.get_config() a = nacos()config = a.get_config()a.add_watcher(&#123;call&#125;) @app.get(&quot;/print&quot;)async def root(): var = 1 / 0 return &#123;&quot;message&quot;: config&#125; @app.get(&quot;/prin&quot;)async def root(): return &#123;&quot;message&quot;: config&#125;","categories":[{"name":"nacos","slug":"nacos","permalink":"https://juggler.fun/categories/nacos/"}],"tags":[{"name":"python","slug":"python","permalink":"https://juggler.fun/tags/python/"},{"name":"fastapi","slug":"fastapi","permalink":"https://juggler.fun/tags/fastapi/"},{"name":"nacos","slug":"nacos","permalink":"https://juggler.fun/tags/nacos/"}]},{"title":"Azure Service Bus 实现定时器以及按序执行任务","slug":"Azure-Service-Bus-实现定时器以及按序执行任务","date":"2022-03-22T10:50:13.000Z","updated":"2022-03-23T03:20:09.466Z","comments":true,"path":"C/Azure-Service-Bus-实现定时器以及按序执行任务/","link":"","permalink":"https://juggler.fun/C/Azure-Service-Bus-%E5%AE%9E%E7%8E%B0%E5%AE%9A%E6%97%B6%E5%99%A8%E4%BB%A5%E5%8F%8A%E6%8C%89%E5%BA%8F%E6%89%A7%E8%A1%8C%E4%BB%BB%E5%8A%A1/","excerpt":"","text":"123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151using System;using System.Threading.Tasks;using Azure.Messaging.ServiceBus;using System.Collections;using Newtonsoft.Json.Linq;using System.Collections.Generic; namespace ServiceBusConsole&#123; internal class Program &#123; // connection string to your Service Bus namespace static string connectionString = &quot;Endpoint=sb://&quot;; // name of your Service Bus topic static string topicName = &quot;actieue&quot;; static Queue&lt;string&gt; a; // number of messages to be sent to the topic private const int taskNumber = 8; static void Main(string[] args) &#123; Random random = new Random(); a = new Queue&lt;string&gt;(); for(int i = 0; i &lt; taskNumber; i++) &#123; JObject jo = new JObject(); jo[&quot;taskId&quot;] = i; jo[&quot;excutionTime&quot;] = random.Next(10); a.Enqueue(jo.ToString()); &#125; ArrayList b = new ArrayList(); b.Add(a.Dequeue()); sendMessage(b).Wait(); get().Wait(); &#125; public static async Task sendMessage(ArrayList msgs) &#123; // the client that owns the connection and can be used to create senders and receivers ServiceBusClient client; // the sender used to publish messages to the topic ServiceBusSender sender; // The Service Bus client types are safe to cache and use as a singleton for the lifetime // of the application, which is best practice when messages are being published or read // regularly. // // Create the clients that we&#x27;ll use for sending and processing messages. client = new ServiceBusClient(connectionString); sender = client.CreateSender(topicName); // create a batch ServiceBusMessageBatch messageBatch = await sender.CreateMessageBatchAsync(); foreach (Object o in msgs) &#123; string msg = (string)o; // try adding a message to the batch ServiceBusMessage m = new ServiceBusMessage(msg); JObject j = JObject.Parse(msg); double d = double.Parse(j[&quot;excutionTime&quot;].ToString()); m.ScheduledEnqueueTime = DateTimeOffset.Now.AddSeconds(d); if (!messageBatch.TryAddMessage(m)) &#123; // if it is too large for the batch throw new Exception($&quot;The message msg is too large to fit in the batch.&quot;); &#125; &#125; try &#123; // Use the producer client to send the batch of messages to the Service Bus topic await sender.SendMessagesAsync(messageBatch); &#125; finally &#123; // Calling DisposeAsync on client types is required to ensure that network // resources and other unmanaged objects are properly cleaned up. messageBatch.Dispose(); await sender.DisposeAsync(); await client.DisposeAsync(); &#125; &#125; public static async Task get() &#123; ServiceBusClient client; ServiceBusProcessor processor; client = new ServiceBusClient(connectionString); processor = client.CreateProcessor(topicName, new ServiceBusProcessorOptions()); try &#123; // add handler to process messages processor.ProcessMessageAsync += MessageHandler; // add handler to process any errors processor.ProcessErrorAsync += ErrorHandler; // start processing await processor.StartProcessingAsync(); Console.WriteLine(&quot;Start Task 0 is runing&quot;); Console.WriteLine($&quot;It is &#123;DateTimeOffset.Now&#125; now&quot;); Console.ReadKey(); // stop processing Console.WriteLine(&quot;\\nStopping the receiver...&quot;); await processor.StopProcessingAsync(); Console.WriteLine(&quot;Stopped receiving messages&quot;); &#125; finally &#123; // Calling DisposeAsync on client types is required to ensure that network // resources and other unmanaged objects are properly cleaned up. await processor.DisposeAsync(); await client.DisposeAsync(); &#125; &#125; static async Task MessageHandler(ProcessMessageEventArgs args) &#123; ArrayList c = new ArrayList(); string body = args.Message.Body.ToString(); try &#123; c.Add(a.Dequeue()); sendMessage(c).Wait(); &#125; catch (Exception ex) &#123; &#125; Console.WriteLine($&quot;At &#123;DateTimeOffset.Now&#125;,This task is finished:\\nReceived: &#123;body&#125; &quot;); // complete the message. messages is deleted from the subscription. await args.CompleteMessageAsync(args.Message); &#125; // handle any errors when receiving messages static Task ErrorHandler(ProcessErrorEventArgs args) &#123; Console.WriteLine(args.Exception.ToString()); return Task.CompletedTask; &#125; &#125;&#125;","categories":[{"name":"C#","slug":"C","permalink":"https://juggler.fun/categories/C/"}],"tags":[{"name":"C#","slug":"C","permalink":"https://juggler.fun/tags/C/"},{"name":"MQ","slug":"MQ","permalink":"https://juggler.fun/tags/MQ/"}]},{"title":"祝安好","slug":"祝安好","date":"2022-03-22T03:17:52.000Z","updated":"2022-03-23T03:24:10.900Z","comments":true,"path":"杂记/祝安好/","link":"","permalink":"https://juggler.fun/%E6%9D%82%E8%AE%B0/%E7%A5%9D%E5%AE%89%E5%A5%BD/","excerpt":"","text":"我想在早 中 晚对你说 早安 午安 晚安可我早已无处寻你 就好像四季春天我见不到你 我只能带着初醒的梦 和每一朵花对话。夏天我见不到你 绿皮红瓤的季节 我只能剪一头短发 把蝉鸣收藏。秋天我见不到你 黄昏细雨里 我只能回忆起你熟悉的味道 让你感知我最澄澈的眷恋。冬天我见不到你 冬夜渐深 细碎过往已让回忆温柔很多 于是在漫天大雪里 绵延不绝的是我对你的祝福。 我最喜欢的是夏夜的磅礴大雨从前我享受飘摇其中的感觉现在我想在其中撑起一把伞 可我如何前去寻你 绝望疯长恣肆无所忌惮 寂寞而眷恋着某个人 如果此生再不能相见 祝你早安 午安和晚安 改自：https://www.zhihu.com/question/34648905","categories":[{"name":"杂记","slug":"杂记","permalink":"https://juggler.fun/categories/%E6%9D%82%E8%AE%B0/"}],"tags":[{"name":"杂记","slug":"杂记","permalink":"https://juggler.fun/tags/%E6%9D%82%E8%AE%B0/"}]},{"title":"饥荒服务器_docker-compose","slug":"饥荒服务器-docker-compose","date":"2022-03-22T02:57:23.000Z","updated":"2022-03-23T03:25:43.769Z","comments":true,"path":"joy/饥荒服务器-docker-compose/","link":"","permalink":"https://juggler.fun/joy/%E9%A5%A5%E8%8D%92%E6%9C%8D%E5%8A%A1%E5%99%A8-docker-compose/","excerpt":"","text":"​使用了大佬的代码 https://github.com/Jamesits/docker-dst-server.git 但是在我的使用过程中一些mod总会报错，所以尝试将base-image改为ubuntu并解决相应错误后解决，由于我服务器的特殊配置，所以不得不将用户改为root： https://gitee.com/dancingjoker/docker-dst-server 我是使用的docker-compose up –build -d 启动，在docker-compose volumes中配置存档位置，其他方式可以视情况使用 另附定期备份命令： 在crontab -e中编写： 113 4 * * * sudo bash /dir/dst/backup.sh 定时可以参考网站：crontab.guru backup.sh: 12#/bin/bashtar -zcPvf /dir/dst/backup/b$(date %Y%m%d%h%m%s).tar.gz /dir/dst/DoNotStarveTogether/Cluster_1 ​","categories":[{"name":"joy","slug":"joy","permalink":"https://juggler.fun/categories/joy/"}],"tags":[{"name":"joy","slug":"joy","permalink":"https://juggler.fun/tags/joy/"},{"name":"docker","slug":"docker","permalink":"https://juggler.fun/tags/docker/"}]},{"title":"NLP","slug":"NLP","date":"2022-03-22T02:41:52.000Z","updated":"2022-03-23T03:23:10.559Z","comments":true,"path":"NLP/NLP/","link":"","permalink":"https://juggler.fun/NLP/NLP/","excerpt":"","text":"[toc] Abbreviation - - [ToL] To learn [ToLM] To learn more [ToLO] To learn optionally (0501) 05 min 01s (h0501) 1 hour 05 min 01s (hh0501) 2 hour 05 min 01s Lecture 1 - Introduction and Word Vectors NLPConvert one-hot encoding to distributed representitions Ont hot can’t represent the relation between word vectors,it is too big Word2vecIgnore the position of word of context Use two vector in one word: centor word context word.softmax function Train the model: gradient descent There is a term to calculate the gradient descent. (39:50-56:40) result is : ToL Review derivation and the following especially. Show some achievement with code(5640-h0516) We can do vector addition, subtraction, multiplication and division, etc. QAWhy are there center word and context word(h0650) To avoid one vector dot product himself in some situation???? Even synonyms can be merged into a vector(h1215) Which is different from lee ,He says synonyms use different. Lecture 2 Word Vectors,Word Senses,and Neural Classifiers Bag models (0245)The model makes the same predictions at each position. Gradient descent (0600)Not usually use because of the big calculation. step size: not too big nor too small stochastic gradient descent SGD TOBELM (0920)Take part of the corpus billion faster. Maybe even get better result. But it is stochastic, either you need sparse matrix update operations to only update certain rows of full embedding matrices U and V, or you need to keep around a hash for vectors.(1344)ToL more details of word2vec(1400) SG use center to predict contextSGNS negative sampling [ToBLO]use logistic function instead of softmax and take sampling of corpus CBOW opposite. Why use two vectors(1500)Sometime it will dot product with itself. [ToL] The first one is positive word and the last is negative word (2800) negative word is being sampled cause the center word will turn up on other occasions, when it does, there will have other sampling, and it will learn step by step. Why not capture co-occurrence counts directly?(2337) SVD(3230) [ToL]https://zhuanlan.zhihu.com/p/29846048 use svd to get lower dimensional representations for words (3451) Count based vs direct prediction(3900) Encoing meaning components in vector differences(3948)This is to make addition subtraction available for word vectors. GloVe (4313) let dot product minus log of the co-occurrence How to evaluate word vectors Intrinsic vs. extrinsic(4756) Analogy evaluation and hyperparameters (intrinsic)(5515)Word vector distances and their correlation with human judgements(5640)Data shows that 300 dimensional word vector is good(5536)The objective function for the GloVe model and What log-bilinear means(5739)Word senses and word sense ambiguity(h0353)One word different mean different vector. then a word can be the sum of them all It will work good but not bad (h1200) the vector is so sparse that you can separate out different senses (h1402) Lecture 3 Gradients by hand(matric calculus) and algorithmically(the backpropagation algorithm) all the math details of doing nerual net learning Need to be learn again, it is not totally understanded.Named Entity Recognition(0530) Simple NER (0636) How the sample model run (0836) update equation(1220) jacobian(1811) Chain Rule(2015) do one example step (2650)hadamard product ToL Reusing Computation(3402) ds&#x2F;dw Forward and backward propagation(5000) An example(5507)a &#x3D; x+y b &#x3D; max(y,z) f &#x3D; ab Compute all gradients at once (h0005) Back-prop in general computation graph(h0800)[ToL] Automatic Differentiation(h1346)Many tools can calculate automaticly. Manual Gradient checking : Numeric Gradient(h1900) Lecture 4 Dependency Parsing Two views of linguistic structureConstituency &#x3D; phrase structure grammar &#x3D; context-free grammars(CFGs)(0331)Phrase structure organizes words into nested constituents Dependency structure(1449)Dependency structure shows which words depend on (modify, attach to,or are arguments of) Why do we need sentence structure?(2205)Can not express meaning by just one word. Prepositional phrase attachment ambiguity.(2422)There is some sentence to show it: San Jose cops kill man with knifeScientists count whales from space The board approved [its acquisition] [by Royal Trustco Ltd.] [of Toronto] [for $27 a share] [at its monthly meeting]. Coordination scope ambiguity(3614)**Shuttle veteran and longtime NASA executive Fred Gregory appointed to board ** Doctor: No heart, cognitive issues Adjectival&#x2F;Adverbial Modifier Ambiguity(3755)Students get [first hand job] experience Students get first [hand job] experience Verb Phrase(VP) attachment ambiguity(4404)Mutilated body washes up on Rio beach to be used for Olympics beach volleyball. Dependency Grammar and Dependency structure(4355) Will add a fake ROOT for handyDependency Grammar history(4742) The rise of annotated data Universal Dependency tree(5100) Tree bank(5400)Its too slow to write a grammar by hand but its still worth,cause it can used in another place but not only nlp . how to build parser with dependency(5738) Dependency Parsing Projectivity(h0416) Methods of Dependency Parsing(h0521) Greedy transition-based parsing(h0621)Basic transition-based dependency parser (h0808) [root] I ate fish [root I ate] fish [root ate] fish [root ate fish] [root ate] [root] MaltParser(h1351)[ToL] Evaluation of Dependency Parsing (h1845)[ToL] Lecture-5 Languages models and Recurrent Neural Networks(RNNs) A neural dependency parser(0624) Distributed Representations(0945) Deep Learning Classifier are non-linear classifiers(1210) Deep Learning Classifier’s non-linear classifiers: Simple feed-forward neural network multi-class classifier (1621) Neural Dependency Parser Model Architecture(1730) Graph-based dependency parsers (2044) Regularization &amp;&amp; Overfitting (2529) Dropout (3100)[ToL] Vectorization(3333) Non-linearities (4000) Parameter Initialization (4357) Optimizers(4617) Learning Rates(4810)It can be slow as the learning go on. Language Modeling (5036) n-gram Language Models(5356) Sparsity Problems (5922)Many situation didn’t occur so it will be zero Storage Problems(h0117)How to build a neural language model(h0609) A fixed-window neural Language Model(h1100) Recurrent Neural Network (RNN)(h1250)x1 -&gt; y1 Wx1 x2 -&gt; y1 A Simple RNN Language Model(h1430) Lecture 6 Simple and LSTM Recurrent Neural Networks. The Simple RNN Language Model (0310) Training an RNN Language Model (0818)RNN takes more time. Teacher Forcingpenalize when dont take its advise But how do we get the answer? Evaluating Language Models (2447)[ToL] Language Model is a system that predicts the next word(3130) Other use of RNN(3229)Tag for word Used for classification(3420) Used to Language encoder module (3500) Used to generate text (3600) Problems with Vanishing and Exploding Gradients(3750)[IMPORTANT] [ToL] Why This is a problem (4400) We can give him a limit. Long Short Term Memory RNNS(LSTMS)(5000)[ToL] Bidirectional RNN (h2000)We need information from the word after Lecture-7 Translation, Seq2Seq, Attention Machine Translation(0245) What do you need (1200)you need parallel corpus,Then you need alignment Decoding for SMT(1748)Try many possible sequences. What is Neural Machine Translation(NMT)(2130)Neural Machine Translation(NMT) is a way to do Machine Translation with a single end-to-end neural net work. The neural network architecture is called sequence-to-sequence model(aka seq2seq) and it involves RNNs Seq2seq is more than MT(2600) (2732)[ToL]Multi-layer RNNs(3323) Lower-level basic meaning Higher-level overall meaning Greedy decoding(4000) Exhaustive search decoding(4200) beam search decoding(4400) How do we evaluate Machine Translation(5550)BLEU NMT perhaps the biggest success story of NLP Deep Learning(h00000)Attention(h1300) Lecture 8 Final Projects; Practical Tips Sequence to Sequence with attention(0235) Attention: in equations(0800) there are several attention variants(1500) Attention is a general Deep Learning technique(2240) Final Project(3000)Lecture-9 Self- Attention and Transformers Issues with recurrent models (0434)Linear interaction distanceSometimes it is too far too learn from the words. Lack of parallelizability(0723)GPU can count parallelizable but RNN lacks that. If not recurrenceWord window models aggregate local contexts (1031) Attention(1406) Self-Attention(1638) Self-attention as an nlp building block(2222) Fix the first self-attention problemsequence order (2423) Position representation vector through sinusoids(2624)Sinusoidal position representations(2730)Position representation vector from scratch(2830) Adding nonlinearities in self-attention(2953)Barriers and solutions for Self-Attention as building block(2945) (3040) (3428) The transformer encoder-decoder(3638) [ToL] key query value(4000) Multi-headed attention (4322)(4450) Residual connections(4723) Layer normalization(5045) Scaled fot product(5415)Lecture 10 - Transformers and Pretraining Word structure and subword models(0300)transform transformerify taaaasty The byte-pair encoding(0659)Subwords model learn the structure of word. The byte-pair between it and dont learn structure. (0943) Motivating word meaning and context(1556) Pretraining whole models(2000) Wordv2vec dont consider context but we can use LSTM to achieve that. Mask some data and pretrain the model with them. this model haven’t met overfitting now, you can save some data to test it.(2811)transformers for encoding and decoding (3030)Pretraining through language modeling(3400) Stochastic gradient descent and pretrain&#x2F;finetune(3740)Model pretraining has three ways (4021) Decoder can see the history, the Encoder can also the future. Encoder-Decoder maybe is the better. Decoder(4300) Generative Pretrained Transformer(GPT) (4818) GPT2(5400) Pretraining Encoding(5545)(Bert)(5654) Bert will mask some words, ask what have I mask Bidirectional encoder representations from transformers(h0100)[ToL] Limitations of pretrained encoders(h0900) Extensions of BERT(h1000) Pretraining Encoder-Decoder (h1200)T5(h1500)The model even dont know how many words are masked In the pretraining the model learned a lot, but it is not always good GPT3(h1800) Lecture 11 Question Answering What is question answering(0414) There are lots of practical applications(0629) Beyond textual QA problems(1100)Reading comprehension(1223) They are useful for many practical applications Reading comprehension is an important tested for evaluating how well computer systems understand human language Standord question answering dataset (1815) Neural models for reading comprehension(2428) LSTM-based vs BERT models (2713) BiDAF(3200) Encoding(3200) Attention(3400) Modeling and output layers(4640) BERT for reading comprehension (5227) Comparisons between BiDAF and BERT models(2734) Can we design better pre-training objectives(h0000) open domain question answering(h1000) DPR(H1400) DensePhrase:Demo(h1800)Lecture 12 - Natural Language Generation[ToL] What is neural language generation?(0300) Mache Translate Dialogue Systems &#x2F;&#x2F;siri Summarization Visual Description Creative Generation &#x2F;&#x2F;story Components of NLG Systems(0845)Basic of natural language generation(0916) A look at a single step(1024) then select and train(1115)teacher forcing need to be leaned Decoding(1317) Greedy methods(1432) Greedy methods get repetitive(1545) why do repetition happen(1613) How can we reduce repetition (1824)[ToL] People is not always choose the greedy methods(1930) Time to get random: Sampling(2047) Decoding : Top-k sampling(2100) Issues with Top-k sampling(2339) Decoding: Top-p(nucleus)sampling(2421) Scaling randomness: Softmax temperature (2500)[ToL] improving decoding: re-balancing distributions(2710) Backpropagation-based distribution re-balancing(3027) Improving Decoding: Re-ranking(3300)[ToL] Decoding: Takeaways(3540) Training NLG models(4114)Maximum Likelihood Training(4200)Are greedy decoders bad because of how they’re trained? Unlikelihood Training(4427)[ToL] Exposure Bias(4513)[ToL] Exposure Bias Solutions(4645) Reinforce Basics(4900) Reward Estimation(5020) reinforce’s dark side(5300) Training: Takeways(5423) Evaluating NLG Systems(5613)Types of evaluation methods for text generation(5734) Content Overlap metrics(5800) A simple failure case(5900) Semantic overlap metrics(h0100) Model-based metrics(h0120) word distance functions(h0234) Beyond word matching(h0350) Human evaluations(h0433) Issues(h0700) Takeways(h0912) Ethical Considerations(h1025) Lecture 13 - Coreference Resolution What is Coreference Resolution?(0604)Identify all mentions that refer to the same entity in the world Applications (1712) Coreference Resolution in Two steps(1947) Mention Detection(2049) Not quite so simple(2255) It is the best donut. I want to find the best donut. Avoiding a traditional pipeline system(2811) End to End[ToL] Onto Coreference! First, some linguistics (3035)Coreference and Anaphor not all anaphoric relations are coreferential (3349) Anaphora vs Cataphora(3610)One look its reference before it the other is after it. Taking stock (3801) Four kinds of coreference Models(4018) Traditional pronominal anaphora resolution:Hobbs’s naive algorithm(4130) Knowledge-based Pronominal Coreference(4820) Hobb’s method can not really solve the questions, the model should really understand the sentence. Coreference Models: Mention Pair(5624) Mention Pair Test Time(5800) Disadvantage(5953) Coreference Models: Mention Ranking(h0050) Convolutional Neural Nets(h0341) What is convolution anyway?(h0452) Summarize what we have usually use pooling Max pooling is usually better. End-to-End Neural Coref Model(h1206) Conclusion (h2017) Lecture 14 - T5 and Large Language Models (0243) T5 with a task prefix(0800) Others STSB Summarize T5 change little from original transformer(1300) what should my pre-training data set be?(1325)Get from open source data source and then wipe them and get c4 1500 Then is how to train from a start(1659) pretrain(1805) choose the model(2412) They use the encoder-Decoder model, It turns out it works well. They dont change hyper paramenters because of the cost pre-training objective(2629) Choose different train method different structure of data source(2822) Multi task learning (3443) close the gap between multi-task training and this pre-training followed by separate fine tuning(3621) What if it happens there are four times computes as much as before (3737) Overview(3840) What about all of the other languages?(mT5)(4735) Same model different corpus. XTREME (5000) How much knowledge does a language model pick up during pre-training?(5225) Salient span masking (5631) Instead of mask randomly, it mask username please date, etc. Do large language models memorize their training data(h0100)It seems it did They need to see examples, they need to see particular examples fewer times in order! Can we close the gap between large and small models by improving the transformer architecture(h1010) in these test, they change some architecture such as RELu. there actually were very few, if any modifications that improved performance meaningfully. (h1700) QA(h1915)Lecture 15 - Add Knowledge to Language Models Recap: LM(0232) What does a language model know?(0423) Thing may right in logic but wrong in fact. The importance of know ledge-aware language models(0700) Query traditional knowledge bases(0750) Query language models as knowledge bases(0955) Compare and disadvantage(1010) Techniques to add knowledge to LMs(130) Add pretrained embeddings(1403) Aside: What is entity linking?(1516) Method 1: Add pretrained entity embeddings(1815) How to we incorporate pretrained entity embeddings from a different embedding space?(2000) ERNIE: Enhanced language representation with informative entities(2143) strengths &amp; remaining challenges(2610) Jointly learn to link entities with KnowBERT(2958) Use an external memory(3140) KGLM(3355) Local knowledge and full knowledge When should the model use the external knowledge(3600) Compare to the others(4334) More recent takes: Nearest Neighbor Language Models(kNN-LM)(4730) Modify the training data(5230) WKLM(5458) Learn inductive biases through masking(5811) Salient span masking(5927) Recap(h0053) Evaluating knowledge in LMS(h0211)LAMA(h0250) The limitations (h0650) LAMA_UnHelpful Names(LAMA-UHN) ** They delete something that may caused by co-occurrence ** Developing better prompts to query knowledge in LMS Knowledge-driven downstream tasks(h1253) Relation extraction performance on TACED(h1400) Entity typing performance on Open Entuty Recap: Evaluating knowledge in LMs(h1600) Other exciting progress &amp; what’s next?(h1652) Lecture 17 - Model Analysis and Explanation Motivationwhat are our models doing(0415) how do we make tomorrow’s model?(0515) What biases are built into model?(0700) how do we make in the following 25years(0800) Model analysis at varying levels of abstraction(0904) Model evaluation as model analysis(1117) Model evaluation as model analysis in natural language inference(1344) What if the model is simple using heuristics to get good accuracy?(1558) Language models as linguistic test subjects(2023) Careful test sets as unit test suites: CheckListing(3230) Fitting the dataset vs learning the task(3500) Knowledge evaluation as model analysis(3642) Input influence: does my model really use long-distance context?(3822) Prediction explanations: what in the input led to this output?(4054) Prediction explanations: simple saliency maps(4230) Explanation by input reduction (4607) Analyzing models by breaking them(5106) They add a nonsense sentence at the end and the prediction changed. Change the Q also make the prediction changed Are models robust to noise in their input?(5518) It seems not. Analysis of “interpretable” architecture components(5719) Probing: supervised analysis of neural networks(h0408) the most efficient layer is in the middlwe. deeper, more abstract Emergent simple structure in neural networks(h1019) Probing: tress simply recoverable from BERT representations(h1136) Final thoughts on probing and correlation studies(h1341) Not causal study Recasting model tweaks and ablations as analysis(h1406) Ablation analysis: do we need all these attension heads?(h1445) What’s the right layer order for a transformer?(h1537) Parting thoughts(h1612) Lecture 18 - Future of NLP + Deep Learning General Representation Learning Recipe(0312) Certain properties emerge only when we scale up the model size! Large Language Models and GPT-3(0358)Large Language models and GPT-3(0514) What’s new about GPT-3","categories":[{"name":"NLP","slug":"NLP","permalink":"https://juggler.fun/categories/NLP/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"https://juggler.fun/tags/NLP/"}]},{"title":"Pytorch","slug":"Pytorch","date":"2022-03-22T02:41:44.000Z","updated":"2022-03-23T03:23:41.569Z","comments":true,"path":"python/Pytorch/","link":"","permalink":"https://juggler.fun/python/Pytorch/","excerpt":"","text":"&#x2F;watch?v&#x3D;c36lUUr864M&amp;t&#x3D;936s 基本操作1234567891011121314151617181920212223242526272829303132import torchimport numpy as npdevice = torch.device(&quot;cpu&quot;)if torch.cuda.is_available(): device = torch.device(&quot;cuda&quot;)x = torch.ones(2,2,dtype=torch.double)x.size()x = torch.rand(2,2,requires_grad=True,device=device)y = torch.rand(2,2,device=device)z = x+y #+-*/z = torch.add(x,y)#add sub muly.add_(x)# sub_ div_ 原地操作x=torch.rand(4,4)x[1][1].item()y=x.view(-1,2) #插眼a = torch.ones(5)a.to(&quot;cuda&quot;)a.add_(13)b = a.numpy()a = np.ones(6)b = torch.from_numpy(a)b.to(device)print(a)print(b) Gradients123456789101112131415161718192021222324252627282930313233343536373839import torchimport numpy as npx = torch.rand(3,requires_grad=True)#***print(x)y = x+2z = y*y*2#z = z.mean()print(z)v = torch.tensor([0.1,1.0,0.001],dtype=torch.float32)z.backward(v)# 插眼：如果不是标量则必须给vecor ***print(x.grad)x = torch.rand(3,requires_grad=True)#dont calculate 插眼#x.requires_grad_(False)#y = x.detach()#with torch.no_grad():# y = x + 2# print(y)weights = torch.ones(4,requires_grad=True)for epoch in range(3): model_output = (weights*3).sum() model_output.backward() print(weights.grad) weights.grad.zero_() #将积累的计算清零 插眼，需要深入理解 ***#optimizer#optimizer = torch.optim.SGD([weights], lr=0.01)#optimizer.step()#optimizer.zero_grad() Backpropagation123456789101112131415import torchimport numpy as npx = torch.tensor(1.0)y = torch.tensor(2.0)w = torch.tensor(1.0,requires_grad=True)y_hat = w * xloss = (y_hat-y)**2print(loss)loss.backward()print(w.grad) Gradient Descent 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import torchimport numpy as np# f = w * x# f = 2 * xx = np.array([1,2,3,4],dtype=np.float32)y = np.array([2,4,6,8],dtype=np.float32)w = 0.0# model predictiondef forward(x): return w*x# loss MSEdef loss(y,y_predicted): return ((y_predicted-y)**2).mean()# gradient# MSE = 1/N * (w*x -y)**2def gradient(x,y,y_predicted): return np.dot(2*x,y_predicted-y).mean()print(f&#x27;prediction before training: f(5) = &#123;forward(5):.3f&#125;&#x27;)# Traininglearning_rate = 0.01n_iters = 10for epoch in range(n_iters): # prediction = forward pass y_pred = forward(x) #loss l = loss(y,y_pred) # gradients dw = gradient(x,y,y_pred) #update weights w-=learning_rate * dw if epoch%1==0: print(f&#x27;epoch &#123;epoch+1&#125;:w = &#123;w:.3f&#125;,loss = &#123;l:.8f&#125;&#x27;) print(f&#x27;Prediction after training: f(5) = &#123;forward(5):.3f&#125;&#x27;) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152import torch# f = w * x# f = 2 * xx = torch.tensor([1,2,3,4],dtype=torch.float32)y = torch.tensor([2,4,6,8],dtype=torch.float32)w = torch.tensor(0.0,dtype=torch.float32,requires_grad=True)# model predictiondef forward(x): return w*x# loss MSEdef loss(y,y_predicted): return ((y_predicted-y)**2).mean()# gradient# MSE = 1/N * (w*x -y)**2def gradient(x,y,y_predicted): return np.dot(2*x,y_predicted-y).mean()print(f&#x27;prediction before training: f(5) = &#123;forward(5):.3f&#125;&#x27;)# Traininglearning_rate = 0.01n_iters = 10for epoch in range(n_iters): # prediction = forward pass y_pred = forward(x) #loss l = loss(y,y_pred) # gradients #dw = gradient(x,y,y_pred) l.backward() #update weights #w-=learning_rate * dw with torch.no_grad(): w -= learning_rate * w.grad #zero gradients w.grad.zero_() if epoch%1==0: print(f&#x27;epoch &#123;epoch+1&#125;:w = &#123;w:.3f&#125;,loss = &#123;l:.8f&#125;&#x27;) print(f&#x27;Prediction after training: f(5) = &#123;forward(5):.3f&#125;&#x27;) Training pipeline Design model (input,output size,forward pass) Construct loss and optimizer Training loop forward pass: compute and prediction backward pass: gradients update weights 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879import torchimport torch.nn as nn# f = w * x# f = 2 * xx = torch.tensor([[1],[2],[3],[4]],dtype=torch.float32)y = torch.tensor([[2],[4],[6],[8]],dtype=torch.float32)w = torch.tensor(0.0,dtype=torch.float32,requires_grad=True)x_test = torch.tensor([5],dtype=torch.float32)n_samples,n_features = x.shapeprint(n_samples,n_features)input_size = n_featuresoutput_size = n_featuresclass LinearRegression(nn.Module): def __init__(self,input_dim,output_dim): super(LinearRegression,self).__init__() self.lin = nn.Linear(input_dim,output_dim) def forward(self,x): return self.lin(x) #model = nn.Linear(input_size,output_size)model = LinearRegression(input_size,output_size)# model predictiondef forward(x): return w*x# loss MSEdef loss(y,y_predicted): return ((y_predicted-y)**2).mean()# gradient# MSE = 1/N * (w*x -y)**2def gradient(x,y,y_predicted): return np.dot(2*x,y_predicted-y).mean()print(f&#x27;Prediction before training: f(5) = &#123;model(x_test).item():.3f&#125;&#x27;)# Traininglearning_rate = 0.1n_iters = 300loss = nn.MSELoss()optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)for epoch in range(n_iters): # prediction = forward pass y_pred = forward(x) #y_pred = model(x) #loss l = loss(y,y_pred) # gradients #dw = gradient(x,y,y_pred) l.backward() #update weights #w-=learning_rate * dw #with torch.no_grad(): # w -= learning_rate * w.grad optimizer.step() #zero gradients #w.grad.zero_() optimizer.zero_grad() if epoch%1==0: [w,b] = model.parameters() print(f&#x27;epoch &#123;epoch+1&#125;:w = &#123;w[0][0].item():.3f&#125;,loss = &#123;l:.8f&#125;&#x27;) print(f&#x27;Prediction after training: f(5) = &#123;model(x_test).item():.3f&#125;&#x27;) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081import torchimport torch.nn as nn# f = w * x# f = 2 * xx = torch.tensor([[1],[2],[3],[4]],dtype=torch.float32)y = torch.tensor([[2],[4],[6],[8]],dtype=torch.float32)#w = torch.tensor(0.0,dtype=torch.float32,requires_grad=True)x_test = torch.tensor([5],dtype=torch.float32)n_samples,n_features = x.shapeprint(n_samples,n_features)input_size = n_featuresoutput_size = n_featuresclass LinearRegression(nn.Module): def __init__(self,input_dim,output_dim): super(LinearRegression,self).__init__() self.lin = nn.Linear(input_dim,output_dim) def forward(self,x): return self.lin(x) #model = nn.Linear(input_size,output_size)model = LinearRegression(input_size,output_size)# model predictiondef forward(x): return w*x# loss MSEdef loss(y,y_predicted): return ((y_predicted-y)**2).mean()# gradient# MSE = 1/N * (w*x -y)**2def gradient(x,y,y_predicted): return np.dot(2*x,y_predicted-y).mean()print(f&#x27;Prediction before training: f(5) = &#123;model(x_test).item():.3f&#125;&#x27;)# Traininglearning_rate = 0.1n_iters = 300loss = nn.MSELoss()optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)for epoch in range(n_iters): # prediction = forward pass #y_pred = forward(x) y_pred = model(x) #loss l = loss(y,y_pred) # gradients #dw = gradient(x,y,y_pred) l.backward() #update weights #w-=learning_rate * dw #with torch.no_grad(): # w -= learning_rate * w.grad optimizer.step() #zero gradients #w.grad.zero_() optimizer.zero_grad() if epoch%1==0: [w,b] = model.parameters() print(f&#x27;epoch &#123;epoch+1&#125;:w = &#123;w[0][0].item():.3f&#125;,loss = &#123;l:.8f&#125;&#x27;) print(f&#x27;Prediction after training: f(5) = &#123;model(x_test).item():.3f&#125;&#x27;) Linear Regression Design model (input,output size,forward pass) Construct loss and optimizer Training loop forward pass: compute and prediction backward pass: gradients update weights 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253import torchimport torch.nn as nnimport numpy as npfrom sklearn import datasetsimport matplotlib.pyplot as plt# 0)prepare datax_numpy,y_numpy = datasets.make_regression(n_samples=100,n_features=1,noise=20,random_state=1)x = torch.from_numpy(x_numpy.astype(np.float32))y = torch.from_numpy(y_numpy.astype(np.float32))y = y.view(y.shape[0],1)# 插眼n_samples,n_features = x.shape# 1)modelinput_size = n_featuresoutput_size = 1model = nn.Linear(input_size,output_size)pr = model(x).detach().numpy()# 2)loss and optimizerlearning_rate = 0.01criterion = nn.MSELoss()optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)# 3)training loopnum_epochs = 200for epoch in range(num_epochs): # forward pss and loss y_predicted = model(x) loss = criterion(y_predicted,y) # backward pass loss.backward() #update optimizer.step() optimizer.zero_grad() if(epoch+1)%10 ==0: print(f&#x27;epoch: &#123;epoch+1&#125;,loss = &#123;loss.item():.4f&#125;&#x27;)#plotpredicted = model(x).detach().numpy()plt.plot(x_numpy,y_numpy,&#x27;ro&#x27;)plt.plot(x_numpy,predicted,&#x27;b&#x27;)plt.show() Logistic Regression 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172import torchimport torch.nn as nnimport numpy as npfrom sklearn import datasetsfrom sklearn.preprocessing import StandardScalerfrom sklearn.model_selection import train_test_split# 0)prepare databc = datasets.load_breast_cancer()x,y = bc.data,bc.targetn_samples,n_features = x.shapex_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=1234)#sclaesc = StandardScaler()x_train = sc.fit_transform(x_train)x_test = sc.transform(x_test)x_train = torch.from_numpy(x_train.astype(np.float32))x_test = torch.from_numpy(x_test.astype(np.float32))y_train = torch.from_numpy(y_train.astype(np.float32))y_test = torch.from_numpy(y_test.astype(np.float32))y_train = y_train.view(y_train.shape[0],1)y_test = y_test.view(y_test.shape[0],1)# 1)model# f = wx+b, sigmod at the endclass LogisticRegression(nn.Module): def __init__(self,n_input_features): super(LogisticRegression,self).__init__() self.linear = nn.Linear(n_input_features,1) def forward(self,x): y_predicted = torch.sigmoid(self.linear(x)) return y_predictedmodel = LogisticRegression(n_features)# 2)loss and optimizerlearning_rate = 0.03criterion = nn.BCELoss()optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)# 3)training loopnum_epoch = 10000for epoch in range(num_epoch): #forward pass and loss y_predicted = model(x_train) loss = criterion(y_predicted,y_train) #backward pass loss.backward() #update optimizer.step() #zero gradients optimizer.zero_grad() if (epoch+1)%10==0: print(f&#x27;epoch:&#123;epoch+1&#125;,loss = &#123;loss.item():.4f&#125;&#x27;)with torch.no_grad(): y_predicted = model(x_test) y_predicted_cls = y_predicted.round() acc = y_predicted_cls.eq(y_test).sum()/float(y_test.shape[0]) print(f&#x27;accuracy = &#123;acc:.4f&#125;&#x27;) Dataset and Dataloader1234567891011121314151617181920212223242526272829303132333435363738394041424344import torchimport torchvisionfrom torch.utils.data import Dataset, DataLoaderimport numpy as npimport mathclass WineDataSet(Dataset): def __init__(self): # data loading xy = np.loadtxt(&quot;K:\\\\Cloud\\\\data\\\\nlp\\\\pytorchTutorial\\\\data\\\\wine\\\\wine.csv&quot;, delimiter=&quot;,&quot;, dtype=np.float32, skiprows=1) self.x = torch.from_numpy(xy[:, 1:]) self.y = torch.from_numpy(xy[:, [0]]) self.n_samplses = xy.shape[0] def __getitem__(self, index): # dataset[0] return self.x[index], self.y[index] def __len__(self): # len(dataset) return self.n_samplsesdataset = WineDataSet()dataloader = DataLoader(dataset=dataset, batch_size=4, shuffle=True, num_workers=0)# datatiter = iter(dataloader)# data = datatiter.next()# features, labels = data# print(features,labels)# training loopnum_epochs = 2total_samples = len(dataset)n_iterations = math.ceil(total_samples / 4)print(total_samples, n_iterations)for epoch in range(num_epochs): for i, (inputs, labels) in enumerate(dataloader): # forward backward, update if (i + 1) % 5 == 0: print(f&#x27;epoch&#123;epoch + 1&#125;/&#123;num_epochs&#125;,step &#123;i + 1&#125;/&#123;n_iterations&#125;,inputs &#123;inputs.shape&#125;&#x27;) Dataset transformers123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354import torchimport torchvisionfrom torch.utils.data import Dataset, DataLoaderimport numpy as npimport mathclass WineDataSet(Dataset): def __init__(self,transform=None): # data loading xy = np.loadtxt(&quot;K:\\\\Cloud\\\\data\\\\nlp\\\\pytorchTutorial\\\\data\\\\wine\\\\wine.csv&quot;, delimiter=&quot;,&quot;, dtype=np.float32,skiprows=1) self.x = xy[:, 1:] self.y = xy[:, [0]] self.n_samplses = xy.shape[0] self.transform = transform def __getitem__(self, index): # dataset[0] sample = self.x[index],self.y[index] if self.transform: sample = self.transform(sample) return sample def __len__(self): # len(dataset) return self.n_samplsesclass ToTensor: def __call__(self,sample): inputs,targets = sample return torch.from_numpy(inputs),torch.from_numpy(targets) class MulTransform: def __init__(self,factor): self.factor = factor def __call__(self,sample): inputs,target = sample inputs *= self.factor return inputs,targetdataset = WineDataSet(transform=None)first_data = dataset[0]features,labels = first_dataprint(features)print(type(features),type(labels))composed = torchvision.transforms.Compose([ToTensor(),MulTransform(2)])dataset = WineDataSet(transform=composed)first_data = dataset[0]features,labels = first_dataprint(features)print(type(features),type(labels)) SoftMax and Crossentropy 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import torchimport torchvisionfrom torch.utils.data import Dataset, DataLoaderimport numpy as npimport mathimport torch.nn as nndef softmax(x): return np.exp(x)/np.sum(np.exp(x),axis=0)x = np.array([2.0,1.0,0.1])outputs = softmax(x)print(&#x27;softmax numpy:&#x27;,outputs)x = torch.tensor([2.0,1.0,0.1])outputs = torch.softmax(x,dim=0)print(outputs)def cross_entropy(actual,predicted): loss = -np.sum(actual * np.log(predicted)) return lossY = np.array([1,0,0])Y_pred_good = np.array([0.7,0.2,0.1])Y_pred_bad = np.array([0.1,0.3,0.6])l1 = cross_entropy(Y,Y_pred_good)l2 = cross_entropy(Y,Y_pred_bad)print(f&#x27;Loss1 numpy:&#123;l1:.4f&#125;&#x27;)print(f&#x27;Loss2 numpy:&#123;l2:.4f&#125;&#x27;)loss = nn.CrossEntropyLoss()#3 samplesY = torch.tensor([2,0,1])y_pred_good = torch.tensor([[0.1,1.0,2.1],[2.0,1.0,0.1],[0.1,3.0,0.1]],dtype=torch.float32)y_pred_bad = torch.tensor([[2.1,1.0,0.1],[0.1,1.0,2.1],[0.1,3.0,0.1]],dtype=torch.float32)l1 = loss(y_pred_good,Y)l2 = loss(y_pred_bad,Y)print(l1.item())print(l2.item())_,pred1 = torch.max(y_pred_good,1)_,pred2 = torch.max(y_pred_bad,1)print(pred1)print(pred2) Neural network 123456789101112131415161718192021import torchimport torch.nn as nn# Multiclass problemclass NeuralNet2(nn.Module): def __init__(self,input_size,hidden_size,num_classes): super(NeuralNet2,self).__init__() self.linear1 = nn.Linear(input_size,hidden_size) self.relu = nn.ReLU() self.linear2 = nn.Linear(hidden_size,num_classes) def forward(self,x): out = self.linear1(x) out = self.relu(out) out = self.linear2(out) y_pred = torch.sigmoid(out) return y_predmodel = NeuralNet2(input_size=28*28,hidden_size=5,num_classes=1)criterion = nn.CrossEntropyLoss() Activation Function123456789101112131415161718192021222324252627282930313233343536import torchimport torch.nn as nnimport torch.nn.functional as F# option1 create nn modulesclass NeuralNet2(nn.Module): def __init__(self,input_size,hidden_size,num_classes): super(NeuralNet2,self).__init__() self.linear1 = nn.Linear(input_size,hidden_size) self.relu = nn.ReLU() #nn.Sigmoid #nn.Softmax #nn.TanH #nn.LeakyReLU self.linear2 = nn.Linear(hidden_size,num_classes) self.sigmoid = nn.Sigmoid() def forward(self,x): out = self.linear1(x) out = self.relu(out) out = self.linear2(out) out = self.sigmoid(out) return out# option 2 use activation functions directly in forward passclass NeuralNet(nn.Module): def __init__(self,input_size,hidden_size): super(NeuralNet,self).__init__() #F.leaky_relu() self.linear = nn.Linear(input_size,hidden_size) self.linear2 = nn.Linear(hidden_size,1) def forward(self,x): out = torch.relu(self.linear1(x)) out = torch.simoid(self.linear2(out)) return out Feed-Forward Neural Net123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687import torchimport torch.nn as nnimport torchvisionimport torchvision.transforms as transformsimport matplotlib.pyplot as plt# device configdevice = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)#hyper parametersinput_size = 784 #28*28hidden_size = 100num_classes = 10num_epochs = 2batch_size = 100learning_rate = 0.001# MNISTtrain_dataset = torchvision.datasets.MNIST(root=&quot;./data&quot;,train=True,transform=transforms.ToTensor(),download=True)test_dataset = torchvision.datasets.MNIST(root=&quot;./data&quot;,train=False,transform=transforms.ToTensor())# DataLoader,Transformationtrain_loader = torch.utils.data.DataLoader(dataset=train_dataset,batch_size=batch_size,shuffle=True)test_loader = torch.utils.data.DataLoader(dataset=test_dataset,batch_size=batch_size,shuffle=False)examples = iter(train_loader)samples,labels = examples.next()print(samples.shape,labels.shape)for i in range(6): plt.subplot(2,3,i+1) plt.imshow(samples[i][0],cmap=&#x27;gray&#x27;)class NeuralNet(nn.Module): def __init__(self,input_size,hidden_size,num_classes): super(NeuralNet,self).__init__() self.l1 = nn.Linear(input_size,hidden_size) self.relu = nn.ReLU() self.l2 = nn.Linear(hidden_size,num_classes) def forward(self,x): out = self.l1(x) out = self.relu(out) out = self.l2(out) return outmodel = NeuralNet(input_size,hidden_size,num_classes)model=model.to(device)## loss and optimizercriterion = nn.CrossEntropyLoss()optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)#training loon_total_steps = len(train_loader)for epoch in range(num_epochs): for i,(images,labels) in enumerate(train_loader): #100,1,28,28 #100,784 images = images.reshape(-1,28*28).to(device) labels = labels.to(device) #forward outputs = model(images) loss = criterion(outputs,labels) #backwards optimizer.zero_grad() loss.backward() optimizer.step() if(i+1)%100==0: print(f&#x27;epoch &#123;epoch+1&#125;/&#123;num_epochs&#125;,step &#123;i+1&#125;.n_ntotal_steps, loss = &#123;loss.item():.4f&#125;&#x27;)# testwith torch.no_grad(): n_correct = 0 n_samples = 0 for images,labels in test_loader: images = images.reshape(-1,28*28).to(device) labels = labels.to(device) outputs = model(images) #value,index _,predictions = torch.max(outputs,1) n_samples+= labels.shape[0] n_correct = (predictions == labels).sum().item() acc = 100.0* n_correct/n_samples print(f&#x27;accuracy = &#123;acc&#125;&#x27;) CNN 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129import torchimport torch.nn as nnimport torch.nn.functional as Fimport torchvisionimport torchvision.transforms as transformsimport matplotlib.pyplot as pltimport numpy as np# Device configurationdevice = torch.device(&#x27;cuda&#x27; if torch.cuda.is_available() else &#x27;cpu&#x27;)# Hyper-parameters num_epochs = 5batch_size = 4learning_rate = 0.001# dataset has PILImage images of range [0, 1]. # We transform them to Tensors of normalized range [-1, 1]transform = transforms.Compose( [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])# CIFAR10: 60000 32x32 color images in 10 classes, with 6000 images per classtrain_dataset = torchvision.datasets.CIFAR10(root=&#x27;./data&#x27;, train=True, download=True, transform=transform)test_dataset = torchvision.datasets.CIFAR10(root=&#x27;./data&#x27;, train=False, download=True, transform=transform)train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)classes = (&#x27;plane&#x27;, &#x27;car&#x27;, &#x27;bird&#x27;, &#x27;cat&#x27;, &#x27;deer&#x27;, &#x27;dog&#x27;, &#x27;frog&#x27;, &#x27;horse&#x27;, &#x27;ship&#x27;, &#x27;truck&#x27;)def imshow(img): img = img / 2 + 0.5 # unnormalize npimg = img.numpy() plt.imshow(np.transpose(npimg, (1, 2, 0))) plt.show()# get some random training imagesdataiter = iter(train_loader)images, labels = dataiter.next()# show imagesimshow(torchvision.utils.make_grid(images))class ConvNet(nn.Module): def __init__(self): super(ConvNet, self).__init__() self.conv1 = nn.Conv2d(3, 6, 5) self.pool = nn.MaxPool2d(2, 2) self.conv2 = nn.Conv2d(6, 16, 5) self.fc1 = nn.Linear(16 * 5 * 5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): # -&gt; n, 3, 32, 32 x = self.pool(F.relu(self.conv1(x))) # -&gt; n, 6, 14, 14 x = self.pool(F.relu(self.conv2(x))) # -&gt; n, 16, 5, 5 x = x.view(-1, 16 * 5 * 5) # -&gt; n, 400 x = F.relu(self.fc1(x)) # -&gt; n, 120 x = F.relu(self.fc2(x)) # -&gt; n, 84 x = self.fc3(x) # -&gt; n, 10 return xmodel = ConvNet().to(device)criterion = nn.CrossEntropyLoss()optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)n_total_steps = len(train_loader)for epoch in range(num_epochs): for i, (images, labels) in enumerate(train_loader): # origin shape: [4, 3, 32, 32] = 4, 3, 1024 # input_layer: 3 input channels, 6 output channels, 5 kernel size images = images.to(device) labels = labels.to(device) # Forward pass outputs = model(images) loss = criterion(outputs, labels) # Backward and optimize optimizer.zero_grad() loss.backward() optimizer.step() if (i+1) % 2000 == 0: print (f&#x27;Epoch [&#123;epoch+1&#125;/&#123;num_epochs&#125;], Step [&#123;i+1&#125;/&#123;n_total_steps&#125;], Loss: &#123;loss.item():.4f&#125;&#x27;)print(&#x27;Finished Training&#x27;)PATH = &#x27;./cnn.pth&#x27;torch.save(model.state_dict(), PATH)with torch.no_grad(): n_correct = 0 n_samples = 0 n_class_correct = [0 for i in range(10)] n_class_samples = [0 for i in range(10)] for images, labels in test_loader: images = images.to(device) labels = labels.to(device) outputs = model(images) # max returns (value ,index) _, predicted = torch.max(outputs, 1) n_samples += labels.size(0) n_correct += (predicted == labels).sum().item() for i in range(batch_size): label = labels[i] pred = predicted[i] if (label == pred): n_class_correct[label] += 1 n_class_samples[label] += 1 acc = 100.0 * n_correct / n_samples print(f&#x27;Accuracy of the network: &#123;acc&#125; %&#x27;) for i in range(10): acc = 100.0 * n_class_correct[i] / n_class_samples[i] print(f&#x27;Accuracy of &#123;classes[i]&#125;: &#123;acc&#125; %&#x27;) Transfer Learning 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185import torchimport torch.nn as nnimport torch.optim as optimfrom torch.optim import lr_schedulerimport numpy as npimport torchvisionfrom torchvision import datasets, models, transformsimport matplotlib.pyplot as pltimport timeimport osimport copymean = np.array([0.5, 0.5, 0.5])std = np.array([0.25, 0.25, 0.25])data_transforms = &#123; &#x27;train&#x27;: transforms.Compose([ transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize(mean, std) ]), &#x27;val&#x27;: transforms.Compose([ transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize(mean, std) ]),&#125;data_dir = &#x27;data/hymenoptera_data&#x27;image_datasets = &#123;x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in [&#x27;train&#x27;, &#x27;val&#x27;]&#125;dataloaders = &#123;x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=0) for x in [&#x27;train&#x27;, &#x27;val&#x27;]&#125;dataset_sizes = &#123;x: len(image_datasets[x]) for x in [&#x27;train&#x27;, &#x27;val&#x27;]&#125;class_names = image_datasets[&#x27;train&#x27;].classesdevice = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)print(class_names)def imshow(inp, title): &quot;&quot;&quot;Imshow for Tensor.&quot;&quot;&quot; inp = inp.numpy().transpose((1, 2, 0)) inp = std * inp + mean inp = np.clip(inp, 0, 1) plt.imshow(inp) plt.title(title) plt.show()# Get a batch of training datainputs, classes = next(iter(dataloaders[&#x27;train&#x27;]))# Make a grid from batchout = torchvision.utils.make_grid(inputs)imshow(out, title=[class_names[x] for x in classes])def train_model(model, criterion, optimizer, scheduler, num_epochs=25): since = time.time() best_model_wts = copy.deepcopy(model.state_dict()) best_acc = 0.0 for epoch in range(num_epochs): print(&#x27;Epoch &#123;&#125;/&#123;&#125;&#x27;.format(epoch, num_epochs - 1)) print(&#x27;-&#x27; * 10) # Each epoch has a training and validation phase for phase in [&#x27;train&#x27;, &#x27;val&#x27;]: if phase == &#x27;train&#x27;: model.train() # Set model to training mode else: model.eval() # Set model to evaluate mode running_loss = 0.0 running_corrects = 0 # Iterate over data. for inputs, labels in dataloaders[phase]: inputs = inputs.to(device) labels = labels.to(device) # forward # track history if only in train with torch.set_grad_enabled(phase == &#x27;train&#x27;): outputs = model(inputs) _, preds = torch.max(outputs, 1) loss = criterion(outputs, labels) # backward + optimize only if in training phase if phase == &#x27;train&#x27;: optimizer.zero_grad() loss.backward() optimizer.step() # statistics running_loss += loss.item() * inputs.size(0) running_corrects += torch.sum(preds == labels.data) if phase == &#x27;train&#x27;: scheduler.step() epoch_loss = running_loss / dataset_sizes[phase] epoch_acc = running_corrects.double() / dataset_sizes[phase] print(&#x27;&#123;&#125; Loss: &#123;:.4f&#125; Acc: &#123;:.4f&#125;&#x27;.format( phase, epoch_loss, epoch_acc)) # deep copy the model if phase == &#x27;val&#x27; and epoch_acc &gt; best_acc: best_acc = epoch_acc best_model_wts = copy.deepcopy(model.state_dict()) print() time_elapsed = time.time() - since print(&#x27;Training complete in &#123;:.0f&#125;m &#123;:.0f&#125;s&#x27;.format( time_elapsed // 60, time_elapsed % 60)) print(&#x27;Best val Acc: &#123;:4f&#125;&#x27;.format(best_acc)) # load best model weights model.load_state_dict(best_model_wts) return model#### Finetuning the convnet ##### Load a pretrained model and reset final fully connected layer.model = models.resnet18(pretrained=True)num_ftrs = model.fc.in_features# Here the size of each output sample is set to 2.# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).model.fc = nn.Linear(num_ftrs, 2)model = model.to(device)criterion = nn.CrossEntropyLoss()# Observe that all parameters are being optimizedoptimizer = optim.SGD(model.parameters(), lr=0.001)# StepLR Decays the learning rate of each parameter group by gamma every step_size epochs# Decay LR by a factor of 0.1 every 7 epochs# Learning rate scheduling should be applied after optimizer’s update# e.g., you should write your code this way:# for epoch in range(100):# train(...)# validate(...)# scheduler.step()step_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)model = train_model(model, criterion, optimizer, step_lr_scheduler, num_epochs=25)#### ConvNet as fixed feature extractor ##### Here, we need to freeze all the network except the final layer.# We need to set requires_grad == False to freeze the parameters so that the gradients are not computed in backward()model_conv = torchvision.models.resnet18(pretrained=True)for param in model_conv.parameters(): param.requires_grad = False# Parameters of newly constructed modules have requires_grad=True by defaultnum_ftrs = model_conv.fc.in_featuresmodel_conv.fc = nn.Linear(num_ftrs, 2)model_conv = model_conv.to(device)criterion = nn.CrossEntropyLoss()# Observe that only parameters of final layer are being optimized as# opposed to before.optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)# Decay LR by a factor of 0.1 every 7 epochsexp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)model_conv = train_model(model_conv, criterion, optimizer_conv, exp_lr_scheduler, num_epochs=25) Tensorboard123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159import torchimport torch.nn as nnimport torchvisionimport torchvision.transforms as transformsimport matplotlib.pyplot as plt############## TENSORBOARD ########################import sysimport torch.nn.functional as Ffrom torch.utils.tensorboard import SummaryWriter# default `log_dir` is &quot;runs&quot; - we&#x27;ll be more specific herewriter = SummaryWriter(&#x27;runs/mnist1&#x27;)#################################################### Device configurationdevice = torch.device(&#x27;cuda&#x27; if torch.cuda.is_available() else &#x27;cpu&#x27;)# Hyper-parameters input_size = 784 # 28x28hidden_size = 500 num_classes = 10num_epochs = 1batch_size = 64learning_rate = 0.001# MNIST dataset train_dataset = torchvision.datasets.MNIST(root=&#x27;./data&#x27;, train=True, transform=transforms.ToTensor(), download=True)test_dataset = torchvision.datasets.MNIST(root=&#x27;./data&#x27;, train=False, transform=transforms.ToTensor())# Data loadertrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)examples = iter(test_loader)example_data, example_targets = examples.next()for i in range(6): plt.subplot(2,3,i+1) plt.imshow(example_data[i][0], cmap=&#x27;gray&#x27;)#plt.show()############## TENSORBOARD ########################img_grid = torchvision.utils.make_grid(example_data)writer.add_image(&#x27;mnist_images&#x27;, img_grid)#writer.close()#sys.exit()#################################################### Fully connected neural network with one hidden layerclass NeuralNet(nn.Module): def __init__(self, input_size, hidden_size, num_classes): super(NeuralNet, self).__init__() self.input_size = input_size self.l1 = nn.Linear(input_size, hidden_size) self.relu = nn.ReLU() self.l2 = nn.Linear(hidden_size, num_classes) def forward(self, x): out = self.l1(x) out = self.relu(out) out = self.l2(out) # no activation and no softmax at the end return outmodel = NeuralNet(input_size, hidden_size, num_classes).to(device)# Loss and optimizercriterion = nn.CrossEntropyLoss()optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) ############## TENSORBOARD ########################writer.add_graph(model, example_data.reshape(-1, 28*28))#writer.close()#sys.exit()#################################################### Train the modelrunning_loss = 0.0running_correct = 0n_total_steps = len(train_loader)for epoch in range(num_epochs): for i, (images, labels) in enumerate(train_loader): # origin shape: [100, 1, 28, 28] # resized: [100, 784] images = images.reshape(-1, 28*28).to(device) labels = labels.to(device) # Forward pass outputs = model(images) loss = criterion(outputs, labels) # Backward and optimize optimizer.zero_grad() loss.backward() optimizer.step() running_loss += loss.item() _, predicted = torch.max(outputs.data, 1) running_correct += (predicted == labels).sum().item() if (i+1) % 100 == 0: print (f&#x27;Epoch [&#123;epoch+1&#125;/&#123;num_epochs&#125;], Step [&#123;i+1&#125;/&#123;n_total_steps&#125;], Loss: &#123;loss.item():.4f&#125;&#x27;) ############## TENSORBOARD ######################## writer.add_scalar(&#x27;training loss&#x27;, running_loss / 100, epoch * n_total_steps + i) running_accuracy = running_correct / 100 / predicted.size(0) writer.add_scalar(&#x27;accuracy&#x27;, running_accuracy, epoch * n_total_steps + i) running_correct = 0 running_loss = 0.0 #################################################### Test the model# In test phase, we don&#x27;t need to compute gradients (for memory efficiency)class_labels = []class_preds = []with torch.no_grad(): n_correct = 0 n_samples = 0 for images, labels in test_loader: images = images.reshape(-1, 28*28).to(device) labels = labels.to(device) outputs = model(images) # max returns (value ,index) values, predicted = torch.max(outputs.data, 1) n_samples += labels.size(0) n_correct += (predicted == labels).sum().item() class_probs_batch = [F.softmax(output, dim=0) for output in outputs] class_preds.append(class_probs_batch) class_labels.append(predicted) # 10000, 10, and 10000, 1 # stack concatenates tensors along a new dimension # cat concatenates tensors in the given dimension class_preds = torch.cat([torch.stack(batch) for batch in class_preds]) class_labels = torch.cat(class_labels) acc = 100.0 * n_correct / n_samples print(f&#x27;Accuracy of the network on the 10000 test images: &#123;acc&#125; %&#x27;) ############## TENSORBOARD ######################## classes = range(10) for i in classes: labels_i = class_labels == i preds_i = class_preds[:, i] writer.add_pr_curve(str(i), labels_i, preds_i, global_step=0) writer.close() ################################################### Save &amp; Load123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137import torchimport torch.nn as nn&#x27;&#x27;&#x27; 3 DIFFERENT METHODS TO REMEMBER: - torch.save(arg, PATH) # can be model, tensor, or dictionary - torch.load(PATH) - torch.load_state_dict(arg)&#x27;&#x27;&#x27;&#x27;&#x27;&#x27; 2 DIFFERENT WAYS OF SAVING# 1) lazy way: save whole modeltorch.save(model, PATH)# model class must be defined somewheremodel = torch.load(PATH)model.eval()# 2) recommended way: save only the state_dicttorch.save(model.state_dict(), PATH)# model must be created again with parametersmodel = Model(*args, **kwargs)model.load_state_dict(torch.load(PATH))model.eval()&#x27;&#x27;&#x27;class Model(nn.Module): def __init__(self, n_input_features): super(Model, self).__init__() self.linear = nn.Linear(n_input_features, 1) def forward(self, x): y_pred = torch.sigmoid(self.linear(x)) return y_predmodel = Model(n_input_features=6)# train your model...####################save all ######################################for param in model.parameters(): print(param)# save and load entire modelFILE = &quot;model.pth&quot;torch.save(model, FILE)loaded_model = torch.load(FILE)loaded_model.eval()for param in loaded_model.parameters(): print(param)############save only state dict ########################## save only state dictFILE = &quot;model.pth&quot;torch.save(model.state_dict(), FILE)print(model.state_dict())loaded_model = Model(n_input_features=6)loaded_model.load_state_dict(torch.load(FILE)) # it takes the loaded dictionary, not the path file itselfloaded_model.eval()print(loaded_model.state_dict())###########load checkpoint#####################learning_rate = 0.01optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)checkpoint = &#123;&quot;epoch&quot;: 90,&quot;model_state&quot;: model.state_dict(),&quot;optim_state&quot;: optimizer.state_dict()&#125;print(optimizer.state_dict())FILE = &quot;checkpoint.pth&quot;torch.save(checkpoint, FILE)model = Model(n_input_features=6)optimizer = optimizer = torch.optim.SGD(model.parameters(), lr=0)checkpoint = torch.load(FILE)model.load_state_dict(checkpoint[&#x27;model_state&#x27;])optimizer.load_state_dict(checkpoint[&#x27;optim_state&#x27;])epoch = checkpoint[&#x27;epoch&#x27;]model.eval()# - or -# model.train()print(optimizer.state_dict())# Remember that you must call model.eval() to set dropout and batch normalization layers # to evaluation mode before running inference. Failing to do this will yield # inconsistent inference results. If you wish to resuming training, # call model.train() to ensure these layers are in training mode.&quot;&quot;&quot; SAVING ON GPU/CPU # 1) Save on GPU, Load on CPUdevice = torch.device(&quot;cuda&quot;)model.to(device)torch.save(model.state_dict(), PATH)device = torch.device(&#x27;cpu&#x27;)model = Model(*args, **kwargs)model.load_state_dict(torch.load(PATH, map_location=device))# 2) Save on GPU, Load on GPUdevice = torch.device(&quot;cuda&quot;)model.to(device)torch.save(model.state_dict(), PATH)model = Model(*args, **kwargs)model.load_state_dict(torch.load(PATH))model.to(device)# Note: Be sure to use the .to(torch.device(&#x27;cuda&#x27;)) function # on all model inputs, too!# 3) Save on CPU, Load on GPUtorch.save(model.state_dict(), PATH)device = torch.device(&quot;cuda&quot;)model = Model(*args, **kwargs)model.load_state_dict(torch.load(PATH, map_location=&quot;cuda:0&quot;)) # Choose whatever GPU device number you wantmodel.to(device)# This loads the model to a given GPU device. # Next, be sure to call model.to(torch.device(&#x27;cuda&#x27;)) to convert the model’s parameter tensors to CUDA tensors&quot;&quot;&quot;","categories":[{"name":"python","slug":"python","permalink":"https://juggler.fun/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"https://juggler.fun/tags/python/"},{"name":"ML","slug":"ML","permalink":"https://juggler.fun/tags/ML/"}]},{"title":"李宏毅-機器學習","slug":"李宏毅-機器學習","date":"2022-03-22T02:41:31.000Z","updated":"2022-03-23T03:22:45.047Z","comments":true,"path":"ML/李宏毅-機器學習/","link":"","permalink":"https://juggler.fun/ML/%E6%9D%8E%E5%AE%8F%E6%AF%85-%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92/","excerpt":"","text":"[toc] https://speech.ee.ntu.edu.tw/\\~hylee/ml/2021-spring.php 預測本頻道觀看人數 (上) - 機器學習基本概念簡介 什么是机器学习机器学习就是让机器具备找一个函数的能力。 例子： 语音识别 图像识别 α GO 不同类型的函数 Regression: The function outputs a scalar. Classification: Given options(classes),the function outputs the correct one, Structured Learning, create something with structure(image,document) How to find a function? A Case Study输入Youtube历史资料输出是第二天的浏览人数 Function with Unknown Parameters Define Loss from Training Data Optimization i 这个图是error surface 步伐取决于斜率和learningrate Hyperparameters 超参数 Gradient descent并不是总能停在global minima 而是停在了local minima. 该问题可以被解决，并不是gradient descent真正的通点(插眼，我猜是计算量太大，引出随机梯度下降) 写一个新的函数，使得预测函数更加复杂更加符合实际（一周为周期） 似乎观看人数的循环规律止于7之28天之间 預測本頻道觀看人數 (下) - 深度學習基本概念簡介 线性model太简单了这种情况叫做Model Bias Sigmoid Function 用多个feature加到一起去近似 引入离散数学： 最终得到 重新定义一些符号。。。 至此我们改进了前面提到的机器学习框架的第一步：Function with unknown课外知识：Hard Sigmoid Loss 先随机找一组θ初始值，以后可以有更好的方法，而不是随机的。之后根据梯度不断更新。 Batch将data随机分为多个batch，每次学习一个batch 每更新一次参数（学习一个batch）叫一次update，每更新完一轮data（学习了多个batch）叫一个epoch。 ReLU 实战效果 多层神经网络 实战效果 此即深度学习Deep &#x3D; Many hidden layers 为什么不将网络变胖而是把它变深， 留待讲解，插眼。Overfitting训练数据loss减少，测试数据loss增大 機器學習任務攻略 Framework of ML General Guide Model bias函数弹性不够大 Optimization Issue 如何判断是model Bias 还是Optimization Issue的原因？ 注意training data上的效果， 多出来的层数不进行操作都可以达到20层的效果，弹性应该比他大得多， 说明优化有问题 test data上出现这种情况可能是由于过拟合，但一定要在training data上验证了才可以确定 Overfitting 测试数据过少，模型预测函数的自由度太大（由于弹性太大）。 解决方案 增加训练资料 Data augmentation Constrained model(根据实际情况降低model的弹性)，限制要适量，过度的话就拟合不出来了。 选mse最低的不一定就是最好的model Cross Validation N-fold Cross Validation Mismatch由于没有将一些情况考虑进去，比如春节，data中不包含春节的经验，所以模型在考虑结果时当然不会考虑到春节的影响。 類神經網路訓練不起來怎麼辦 (一)： 局部最小值 (local minima) 與鞍點 (saddle point)Optimization Fails because…… 当 gradient 接近零，学习速度也就接近0，其原因有可能为： local minima saddle point critical point 如何确定到底是哪个原因？Tayler Series Approximation 可以根据二阶导数来判断：[ToL] 实际上用到的机会少，因为随着模型规模的增大，二阶倒数太难算了 Saddle Point v.s. Local Minima低维的local minima很有可能是高维的saddle point Eigen value 为负的话还是有路可以降低loss 類神經網路訓練不起來怎麼辦 (二)： 批次 (batch) 與動量 (momentum) Review 每次epoch重新分batch，每次这样的操作叫一次epoch Small Batch vs Large Batch 考虑到GPU平行运算的问题更大的batch不一定就比小的batch花的时间多 更新参数所拥有的数据越多，更新越精准，batch越多，更新一次batch的数据越少，噪音越大，然而效果反而更好。 即使train data效果差不多，在test data里小的batch size也会得到更好的效果。 总结 Momentum 所以有种说法说惯性受过去所有运动的影响。 類神經網路訓練不起來怎麼辦 (三)：自動調整學習速率 (Learning Rate) 训练卡住了，loss不再下降，并不意味着到了local minima 或者鞍点之类的 这样的点叫Critical point But training can be difficult even without critical points 步伐(Learning rite)太大会在两边回荡，即critical point, 但是步伐太小又会使学习缓慢 Different Parameters needs different learning rate RMSProp Adam Learning Rate Scheduling随着终点的临近让学习率下降（Learning rate decay） Residual Network在transformer中引用了warm up的方法 Summary of Optimization 两个都考虑所有历史，但是一个更注重方向，一个只注重大小。 Next Time 類神經網路訓練不起來怎麼辦 (四)：損失函數 (Loss) 也可能有影響To learn more Classification as Regression? 暗示12关系比较近，13比较远，所以不是很可行，所以选择one-hot编码。 SoftMax 将可以为任何值的数值映射到0～1之间 当只有两类时softmax和sigmoid是相同的 （插眼，没太懂） Loss of Classification Minimizing cross-entropy is equivalent to maximizing likelihood. Pytorch Cross-entropy 内含Softmax 为什么相较于MSE Cross-entropy更常被用到MSE计算loss不容易从loss大的地方走下来，因为那里梯度太小了 類神經網路訓練不起來怎麼辦 (五)： 批次標準化 (Batch Normalization) 簡介 Changing Landscape 上图很6 Feature NormalizationFeature Normalization是上图的解决办法，以下是Feature Normalization的一种方法 In general, feature normalization makes gradient scent converge faster. Considering Deep Learning Batch Normalization[插眼，没看太懂] BN in Test在训练时先将测试时没有的参数算出来 Internal Covariate Shift?How Does Batch Normalization help Optimization 卷積神經網路 (Convolutional Neural Networks, CNN)Image Classification Observation 1 &#x2F;&#x2F;引出receptive fieldy隐藏层一个节点观测图片的一小部分 Simplification 1receptive field Can different neurons have different sizes of receptive field? Cover only some channels. Not square receptive field? Typical SettingKernel size stride overlap padding Observation 2 &#x2F;&#x2F;引出filter Simplification 2两个节点照顾的位置不一样，但是参数是一样的 Typical Setting Benefit of Convolutional Layer 弹性逐渐减小 Convolutional layer &#x2F;&#x2F;另一种说法 虽然只有3*3，但是后面的层的节点考虑到的会更大 Comparison of Two Stories Observation 3 &#x2F;&#x2F;引出pooling Pooling 把图片变小 最主要的理由是减少运算量 The Whole CNNFlatten 把所有数值拉直变成向量 Application: Playing Go下围棋是一个分类的问题 α Go每个棋盘上的位置都有48个属性 Why CNN for GO playing上面讲的observation跟围棋有相似性。 但要注意下围棋不适合用pooling More Applications To learn more 自注意力機制 (Self-attention) (上)regression输出是一个数值 输入是一个向量 classification 输出是一个类别 输入是一个向量 如果更复杂？ Vector as Input 语音，社交网络，分子结构等可以转化为多个向量作为输入 What is the output?输入数量与输出数量一致 输入输出数量不一致 Sequence Labeling输入与输出一样多 I saw a saw 不能用fully-connected network，因为同一个词汇出现两次对于fully-connected network 来说是一样的,所以要用窗口， 但是由于输入长度不一定，窗口也就不一定，由此引出self -attention Self-attention Part of attention network 谁的关联性更大，其向量就会更占支配地位，b1就会更像谁 自注意力機制 (Self-attention) (下) Multi-head self-attention得到qi,ki,vi后再乘两个矩阵得到两个结果。 Positional Encoding之前的公式没有结合位置信息 这里每个位置的向量是订好的 后来有了新的动态生成的办法 Application Speech用于语音识别时要有所更改，因为语音识别所生成的向量太大了，如果结合所有输入的话计算量可能接受不了。 Image Self-attention for Graph To Learn more about GNN Self-attention v.s. CNNCNN 可以看作Self-attention的子集， 资料少时用CNN，多时用self-attention，因为self-attention弹性更大（插眼，为啥？），需要的资料更多。 Self-attention vs RNNRNN： 很难考虑远处的信息 不是平行的 self-attention: 相反 To learn more about RNN To Learn More Transformer(上) EncoderSequence-to-sequence’s applicationtransformer 是一个Sequence-to-sequence(Swq2seq) model 输出长度是由模型决定的 由闽南语音直接转为汉字 在翻译倒装句时错误率会更高 Seq2seq for ChatbotQuestion Answering 可以理解为Seq2seq model 的问题 Seq2seq for Syntactic parsing输入句子输出文法分析树 将文法视作一种语言用翻译的模型得到结果 seq2seq for multi-label classification Seq2Seq for Object Detection Seq2seq Encoder 先讲个其他的，再回来进行比对 Block原来更加复杂： batch normalization:对不同example 不同feature同一dimension 计算mean 和 standard deviation，在这里没有用到，用到的是layer normalization,他是对不同统一example 同一feature的不同dimension计算mean 和standard deviation. 最后回到encoder的结构，其实是一样的 To learn more Transformer (下) Decoder Autoregressive(Speech Recognition as example)(AT)一步错步步错 Decoder 除了篮框这部分其他的和encoder很像，除了multi-attention 加了mask Masked multi-head attention在计算b2时没办法把考虑a3，a4，因为还没生成出来，模型是从左至右计算的 Stop Token: We don’t know the correct output length Non-autoregressive(NAT)优势在于平行输出，所以时间会变快。而且可以控制输出长度。 To learn more about nat Encode-Decoder Cross Attention train使用已经有结果的数据 给正确答案，希望输出越接近越好 Teacher Forcing tips about train seq2seqCopy Mechanism To learn more Summarization Guided Attention强迫attention有一定固定的行为，比如语音识别必须由左向右 Beam Search尝试多种可能性 有争议，很多人说很烂 因为有可能说重复的话，但是加入一点杂音反而会好很多，说明分数最高的路不一定就是最好的答案。 有明确答案的任务效果更好，需要发散思路的问题可能更需要加入杂音。 另外，tts需要加入杂音才能更好的产生结果 Optimizing Evaluation Metrics?训练用Cross entry评价用BLEU score. 训练和测试不一致（Exposure bias）训练永远看到的是正确的东西,测试会有错的 Scheduled Sampling训练时给点错误的 生成式對抗網路 (Generative Adversarial Network, GAN) (一) – 基本概念介紹Network as Generator 两个结合输出一个新的分布。 为什么要输出一个分布？由于问题的发散产生了分支（过于发散），这时往往会需要输出一个分支 训练中可能会出现同时向左向右转的现象 GANAnime Face Generation Discriminator Basic Idea of GAN两相竞争 Algorithmdiscriminator不断的将生成的和实际的图片分类出来，generator为了不被分辨出来儿不断进步 Progressive GAN 生成式對抗網路 (Generative Adversarial Network, GAN) (二) – 理論介紹與WGANOur Objective But it is too hard to compute the divergence How to solve the problem of divergency(sample and discriminator) DiscriminatorD* 与divergence 相关 can we use other divergence? Tips of ganJS divergence is not suitable 如果取样太少的话，命名generator已经取得了进步但是无法在discriminator 体现不出来 只要没有相交js divergence就为log2，但是有可能已经进步了，只不过没有达到那个程度。 Wasserstein distance让一个分布与另一个分布重合所用的精力 但是当分布复杂时，想让它们重合有不同的moving plan,所以需要穷举。 WGAN 生成式對抗網路 (Generative Adversarial Network, GAN) (三) – 生成器效能評估與條件式生成GAN is still challenging如果一方停下了，没办法再前进的话，另一方也会停下。 GAN for Sequence Generation如果有多个输出且去max的话，那么其他输出的参数因参数的变化而增长是无法体现出来的 For more 为什么用GAN因为GAN目前效果比VAE FLOW好。。。即使是它比较难train也比其他的方法也不会难太多 Possible SolutionTrain一个输入向量，输出图片的模型 Quality of Image Diversity - Mode CollapseDiscriminator万一有弱点被generator抓到的话。。。 Diversity - Mode Dropping看似分布和质量都合理，但是其实真实数据比这更大 用分类器分类的结果如果过去集中则可能是这个问题 如果够平均则可能没有这个问题。 Frechet inception Distance(FID)(插眼) We don’t want memory GAN产生的跟训练资料的一模一样是不行的 Conditional GenerationText to image Application 生成式對抗網路 (Generative Adversarial Network, GAN) (四) – Cycle GANLearning from Unpaired Data（无监督） 没有成对的资料来训练 由于有了还原，产生的图片就不能和输入差太多（保证有一些关系（即使很奇怪（暂时还没啥好解法））） more SELFIE2ANIME Text Style Transfer Other 自督導式學習 (Self-supervised Learning) (一) – 芝麻街與進擊的巨人 自督導式學習 (Self-supervised Learning) (二) – BERT簡介Self-supervised Learning Masking Input Next Sentence Prediction分辨两个句子是不是该接在一起 被认为不是很有用，model没有学到很多东西 Downstream Tasks 将前面的训练结果用在其他训练上 GLUE How to use BERTCase 1 input asq output class Case 2 input seq output same as seq Case 3 input two seq output class Case 4 input document and query output answer Training BERT is challenging BERT Embryology(胚胎学)了解BERT学习到知识的细节 Pre-training a seq2seq model 输入encoder弄坏的数据，decoder输出没坏的数据 自督導式學習 (Self-supervised Learning) (三) – BERT的奇聞軼事Why does BERT work同一个字有不同的意义 一个词汇的意思取决于它的上下文 Applying BERT to protein,DNA,music classification Multi-lingual BERT训练在英文反而在中文的test上取得了进步 Cross-lingual Alignment(一种解释) 似乎不同语言向量的差异就是语言的信息？ 自督導式學習 (Self-supervised Learning) (四) – GPT的野望如下，只不过是数据量特别大 How to use GPT Few-shot learning (no gradient descent) “In-context” Learning只用很少的资料去训练。效果见仁见智 Beyond Text Image Speech语音方面暂时没有公认的像GLUE的资料库 他自己做了一个 Application of self supervised 自編碼器 (Auto-encoder) (上) – 基本概念Outline review of self-supervised learning frame work学习没有标注资料的任务，在有bert gpt之前，就有了auto-encoder。 auto-encoder in imageDimension reduction for more Why auto-encoder? 并不是3*3的向量都是图片，其形式是有限的，所以可以用更小的维度表示3*3的图片。 Auto-encoder is not a new idea De-noising Auto-encoder 自編碼器 (Auto-encoder) (下) – 領結變聲器與更多應用Feature Disentangle分解中间向量，理解其信息 Representation includes information of different aspects Application: voice conversion不需要资料库中两个人说一样的话。 discrete representation 固定输出的可能性，不是无限的而是离散的 Text as representation 强迫encoder train出人话一样的中间向量。 Tree as Embedding Generator Compression Anomaly Detection 用来探测异常交易，异常请求，异常病情等 和分类器还是有区别的，因为训练资料大多数只有一类 For More 來自人類的惡意攻擊 (Adversarial Attack) (上) – 基本概念Motivation需要在有人试图欺骗他的情况下正常工作 How to arrackExample of attack 正常的错误应该是这样的 含恶意的噪音尽量让正确的目标概率变小，让目标概率变大，同时尽量让差距小于人类能感知的差异的最小值 Non-perceivable Attack Approach 來自人類的惡意攻擊 (Adversarial Attack) (下) – 類神經網路能否躲過人類深不見底的惡意？White Box vs Black Box Black Box 不同的数据产生的结果截然不同 One pixel attack只改变一点导致结果改变 Universal Adversarial Attack Beyond Images Attack in the Physical World Backdoor in Model Solution模糊化，让攻击信号被改变 但是如果被知道了使用了这种方法，攻击时也可以加上这个，所以可以加上随机性 Proactive Defense自己攻击自己然后把攻击数据学习进去 機器學習模型的可解釋性 (Explainable ML) (上) – 為什麼類神經網路可以正確分辨寶可夢和數碼寶貝呢？Why we need Explainable ML? Interpretable vs powerful Goal of Explainable ML 人们只是需要一个理由去接受 - .- Explainable ML Local Exception改造或删除某部分导致结论错误，那么它就是原因 更改向量的值，计算偏导数，得出是哪个参数更重要 Saliency Map A test 结果是由于数据错误，，，背景问题 因为介绍文字而判断图片 SmoothGrad Gradient Saturation如果在平滑处取导数的话会得出鼻子长度与判读大象无关的结论 How a net work process the input data为了观察过程压缩中间向量的维度 很多问题尚待研究。。 用探针处理中间向量但注意Classifier的正确情况 example 在不同层加探针看语音到底在那里失去了性别信息，或者杂音。 機器學習模型的可解釋性 (Explainable ML) (下) –機器心中的貓長什麼樣子？What does a filter detect?（插眼，他在说什么）通过filter观察模型在观察什么 图示是某个filter 下图是被攻击了，命名看不出来是什么，机器却觉得是0123456789。 要得到如下的结果要大量的知识和处理。 或者通过generator达到目的（插眼，他在说什么） 并不是在乎机器真正的注意点，而是将注意点转成人能理解的形式 Outlook用简单的linear模型模拟复杂的deep learning模型，再用简单的模型理解复杂的模型 概述領域自適應 (Domain Adaptation)Domain shift有一些改变就会犯错 需要考虑两种情况 Basic idea Domain Adversarial Training Domain Classifier用来分辨是黑白的还是彩色的，训练到它分别不出来且loss不再下降 Limitation(插眼) Outlook有两者数据不完全重合的情况 More condition in domain Adaptationlittle unlabel Know nothing Domain Generation","categories":[{"name":"ML","slug":"ML","permalink":"https://juggler.fun/categories/ML/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"https://juggler.fun/tags/NLP/"},{"name":"ML","slug":"ML","permalink":"https://juggler.fun/tags/ML/"}]},{"title":"Hello World","slug":"hello-world","date":"1970-01-01T00:00:00.000Z","updated":"2022-03-22T02:39:23.252Z","comments":true,"path":"uncategorized/hello-world/","link":"","permalink":"https://juggler.fun/uncategorized/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}],"categories":[{"name":"language","slug":"language","permalink":"https://juggler.fun/categories/language/"},{"name":"data","slug":"data","permalink":"https://juggler.fun/categories/data/"},{"name":"Unity","slug":"Unity","permalink":"https://juggler.fun/categories/Unity/"},{"name":"杂记","slug":"杂记","permalink":"https://juggler.fun/categories/%E6%9D%82%E8%AE%B0/"},{"name":"js","slug":"js","permalink":"https://juggler.fun/categories/js/"},{"name":"python","slug":"python","permalink":"https://juggler.fun/categories/python/"},{"name":"密码学","slug":"密码学","permalink":"https://juggler.fun/categories/%E5%AF%86%E7%A0%81%E5%AD%A6/"},{"name":"nacos","slug":"nacos","permalink":"https://juggler.fun/categories/nacos/"},{"name":"C#","slug":"C","permalink":"https://juggler.fun/categories/C/"},{"name":"joy","slug":"joy","permalink":"https://juggler.fun/categories/joy/"},{"name":"NLP","slug":"NLP","permalink":"https://juggler.fun/categories/NLP/"},{"name":"ML","slug":"ML","permalink":"https://juggler.fun/categories/ML/"}],"tags":[{"name":"janpanese","slug":"janpanese","permalink":"https://juggler.fun/tags/janpanese/"},{"name":"data","slug":"data","permalink":"https://juggler.fun/tags/data/"},{"name":"Unity","slug":"Unity","permalink":"https://juggler.fun/tags/Unity/"},{"name":"杂记","slug":"杂记","permalink":"https://juggler.fun/tags/%E6%9D%82%E8%AE%B0/"},{"name":"js","slug":"js","permalink":"https://juggler.fun/tags/js/"},{"name":"python","slug":"python","permalink":"https://juggler.fun/tags/python/"},{"name":"azure","slug":"azure","permalink":"https://juggler.fun/tags/azure/"},{"name":"MongoDB","slug":"MongoDB","permalink":"https://juggler.fun/tags/MongoDB/"},{"name":"C#","slug":"C","permalink":"https://juggler.fun/tags/C/"},{"name":"密码学","slug":"密码学","permalink":"https://juggler.fun/tags/%E5%AF%86%E7%A0%81%E5%AD%A6/"},{"name":"fastapi","slug":"fastapi","permalink":"https://juggler.fun/tags/fastapi/"},{"name":"nacos","slug":"nacos","permalink":"https://juggler.fun/tags/nacos/"},{"name":"MQ","slug":"MQ","permalink":"https://juggler.fun/tags/MQ/"},{"name":"joy","slug":"joy","permalink":"https://juggler.fun/tags/joy/"},{"name":"docker","slug":"docker","permalink":"https://juggler.fun/tags/docker/"},{"name":"NLP","slug":"NLP","permalink":"https://juggler.fun/tags/NLP/"},{"name":"ML","slug":"ML","permalink":"https://juggler.fun/tags/ML/"}]}